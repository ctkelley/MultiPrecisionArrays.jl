<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Half Precision and Krylov-IR · MultiPrecisionArrays.jl</title><meta name="title" content="Half Precision and Krylov-IR · MultiPrecisionArrays.jl"/><meta property="og:title" content="Half Precision and Krylov-IR · MultiPrecisionArrays.jl"/><meta property="twitter:title" content="Half Precision and Krylov-IR · MultiPrecisionArrays.jl"/><meta name="description" content="Documentation for MultiPrecisionArrays.jl."/><meta property="og:description" content="Documentation for MultiPrecisionArrays.jl."/><meta property="twitter:description" content="Documentation for MultiPrecisionArrays.jl."/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">MultiPrecisionArrays.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><span class="tocitem">Half Precision and Krylov-IR</span><ul><li class="is-active"><a class="tocitem" href>Half Precision and Krylov-IR</a><ul class="internal"><li><a class="tocitem" href="#Half-Precision-is-Subtle"><span>Half Precision is Subtle</span></a></li><li><a class="tocitem" href="#GMRES-IR"><span>GMRES-IR</span></a></li><li><a class="tocitem" href="#BiCGSTAB-IR"><span>BiCGSTAB-IR</span></a></li></ul></li></ul></li><li><span class="tocitem">More than you want to know</span><ul><li><a class="tocitem" href="../Details/Termination/">Terminating the while loop</a></li><li><a class="tocitem" href="../Details/N2Work/">Is O(N^2) work negligible?</a></li><li><a class="tocitem" href="../Details/Interprecision_1/">Interprecision Transfers: Part I</a></li><li><a class="tocitem" href="../Details/Extended/">Evaluating residuals in higher precision</a></li></ul></li><li><span class="tocitem">MPArray Constructors</span><ul><li><a class="tocitem" href="../functions/MPArray/">MPArray: constructor</a></li><li><a class="tocitem" href="../functions/MPGArray/">MPGArray: constructor</a></li><li><a class="tocitem" href="../functions/MPBArray/">MPBArray: constructor</a></li></ul></li><li><span class="tocitem">Factorizations</span><ul><li><a class="tocitem" href="../functions/hlu!/">hlu!: Get LU to perform reasonably well for Float16</a></li><li><a class="tocitem" href="../functions/mplu!/">mplu!: Simple MPArray factorization</a></li><li><a class="tocitem" href="../functions/mplu/">mplu: Combine MPArray construction and factorization</a></li><li><a class="tocitem" href="../functions/mpglu!/">mpglu!: Factor a MPGArray and set it up for GMRES by allocating room for Krylov vectors etc</a></li><li><a class="tocitem" href="../functions/mpglu/">mpglu: Combine MPGArray construction and factorization</a></li><li><a class="tocitem" href="../functions/mpblu!/">mpblu!: Factor a MPBArray and set it up for BiCGSTAB by allocating room for a few vectors</a></li><li><a class="tocitem" href="../functions/mpblu/">mpblu: Combine MPBArray construction and factorization</a></li></ul></li><li><span class="tocitem">Iteration Statistics</span><ul><li><a class="tocitem" href="../Details/Stats/">Harvesting Iteration Statistics</a></li></ul></li><li><span class="tocitem">Solvers</span><ul><li><a class="tocitem" href="../functions/mpgeslir/">mpgeslir: IR solver</a></li><li><a class="tocitem" href="../functions/mpkrir/">mpkrir: Krylov-IR solver</a></li></ul></li><li><span class="tocitem">References</span><ul><li><a class="tocitem" href="../References/">References</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Half Precision and Krylov-IR</a></li><li class="is-active"><a href>Half Precision and Krylov-IR</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Half Precision and Krylov-IR</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/ctkelley/MultiPrecisionArrays.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/ctkelley/MultiPrecisionArrays.jl/blob/main/docs/src/Half_1.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Half-Precision-and-Krylov-IR"><a class="docs-heading-anchor" href="#Half-Precision-and-Krylov-IR">Half Precision and Krylov-IR</a><a id="Half-Precision-and-Krylov-IR-1"></a><a class="docs-heading-anchor-permalink" href="#Half-Precision-and-Krylov-IR" title="Permalink"></a></h1><p>There are two half-precision (16-bit) formats. Julia has native support for IEEE 16-bit floats (Float16). A second format (BFloat16) has a larger exponent field and a smaller  significand (mantissa), thereby trading precision for range. In fact, the exponent field in BFloat is the same size (8 bits) as that for single precision (Float32). The  significand, however, is only 8 bits. Compare this to the size of the exponent fields for  Float16 (5 bits) and single (8 bits). The size of the significand means that you can get in real trouble with half precision in either format.</p><p>At this point Julia has no native support for BFloat16. Progress is being made and the package <a href="https://github.com/JuliaMath/BFloat16s.jl">BFLoat16s</a> will let you experiment.</p><p>Using half precision will not speed anything up, in fact it will make  the solver slower. The reason for this is that LAPACK and the BLAS  do not (<strong>YET</strong>) support half precision, so all the clever stuff in there is missing. We provide a half-precision LU factorization for Float16  <strong>/src/Factorizations/hlu!.jl</strong> that is better than nothing.  It&#39;s a hack of Julia&#39;s  <code>generic_lu!</code> with threading and a couple compiler directives. Even so, it&#39;s 2.5–8 x <strong>slower</strong> than a  double precision LU. Half precision support for both Float16 and BFloat16 is coming  (Julia and Apple support Float16 in hardware! Apple hardware supports BFloat16). However, for now, at least for desktop computing, half precision is for research in iterative refinement, not applications. </p><p>Here&#39;s a table that illustrates the point. In the table we compare timings for LAPACK&#39;s LU to the LU we compute with <code>hlu!.jl</code>. The matrix is  <span>$I-800.0*G$</span>.</p><pre><code class="nohighlight hljs">      N       F64       F32       F16     F16/F64 
     1024  3.96e-03  2.70e-03  2.47e-02  6.22e+00 
     2048  2.27e-02  1.42e-02  6.95e-02  3.06e+00 
     4096  1.54e-01  8.49e-02  3.08e-01  1.99e+00 
     8192  1.14e+00  5.98e-01  4.42e+00  3.88e+00 </code></pre><p>The columns of the table are the dimension of the problem, timings for double, single, and half precision, and the ratio of the half precision timings to double. The timings came from Julia 1.10.2 running on an Apple M2 Pro with 8 performance cores.</p><p>I am constantly playing with <code>hlu!.jl</code> and these timings will almost certainly be different if you try to duplicate them.</p><h2 id="Half-Precision-is-Subtle"><a class="docs-heading-anchor" href="#Half-Precision-is-Subtle">Half Precision is Subtle</a><a id="Half-Precision-is-Subtle-1"></a><a class="docs-heading-anchor-permalink" href="#Half-Precision-is-Subtle" title="Permalink"></a></h2><p>Half precision is also difficult to use properly. The low precision can  make iterative refinement fail because the half precision factorization  can have a large error. Here is an example to illustrate this point.  The matrix here is modestly ill-conditioned and you can see that in the  error from a direct solve in double precision.</p><p>You may not get exactly the same results for this example on different hardware, BLAS, versions of Julia or this package. I am still playing with the termination criteria and the iteration count could grow or shrink as I do that.</p><pre><code class="nohighlight hljs">julia&gt; A=I - 800.0*G;

julia&gt; x=ones(N);

julia&gt; b=A*x;

julia&gt; xd=A\b;

julia&gt; norm(b-A*xd,Inf)
6.96332e-13

julia&gt; norm(xd-x,Inf)
2.30371e-12</code></pre><p>Now, if we downcast things to half precision, nothing good happens.</p><pre><code class="nohighlight hljs">julia&gt; AH=Float16.(A);

julia&gt; AHF=hlu!(AH);

julia&gt; z=AHF\b;

julia&gt; norm(b-A*z,Inf)
6.25650e-01

julia&gt; norm(z-xd,Inf)
2.34975e-01</code></pre><p>So you get very poor, but unsurprising, results. While <strong>MultiPrecisionArrays.jl</strong> supports half precision and I use it all the time, it is not something you  should use in your own work without looking at the literature and making certain you are prepared for strange results. Getting good results consistently from half precision is an active research area.</p><p>So, it should not be a surprise that IR also struggles with half precision. We will illustrate this with one simple example. In this example high precision will be single and low will be half. Using {\bf MPArray} with a single precision matrix will automatically make the low precision matrix half precision.</p><pre><code class="nohighlight hljs">julia&gt; N=4096; G=800.0*Gmat(N); A=I - Float32.(G);

julia&gt; x=ones(Float32,N); b=A*x;

julia&gt; MPF=mplu(A; onthefly=false);

julia&gt; y=MPF\b;

julia&gt; norm(b - A*y,Inf)
1.05272e+02</code></pre><p>So, IR completely failed for this example. We will show how to extract the details of the iteration in a later section.</p><p>It is also worthwhile to see if doing the triangular solves on-the-fly (MPS) helps. </p><pre><code class="nohighlight hljs">julia&gt; MPBF=mplu(A);

julia&gt; z=MPBF\b;

julia&gt; norm(b-A*z,Inf)
1.28174e-03</code></pre><p>So, MPS is better in the half precision case. Moreover, it is also less costly thanks to the limited support for half precision computing. For that reason, MPS is the default when high precision is single.</p><p>However, on-the-fly solves are not enough to get good results and IR still terminates too soon.</p><h2 id="GMRES-IR"><a class="docs-heading-anchor" href="#GMRES-IR">GMRES-IR</a><a id="GMRES-IR-1"></a><a class="docs-heading-anchor-permalink" href="#GMRES-IR" title="Permalink"></a></h2><p>GMRES-IR solves the correction equation with a preconditioned GMRES (<a href="../References/#gmres">Saad and Schultz, 1986</a>) iteration. One way to think of this is that the solve in the IR loop is an approximate solver for the correction equation</p><p class="math-container">\[A d = r\]</p><p>where one replaces <span>$A$</span> with the low precision factors <span>$LU$</span>. In GMRES-IR one solves the correction equation with a left-preconditioned GMRES iteration using <span>$U^{-1} L^{-1}$</span> as the preconditioner. The preconditioned equation is</p><p class="math-container">\[U^{-1} L^{-1}  A d = U^{-1} L^{-1} r.\]</p><p>GMRES-IR will not be as efficient as IR because each iteration is itself an GMRES iteration and application of the preconditioned matrix-vector product has the same cost (solve + high precision matrix vector product) as a single IR iteration. However, if low precision is half, this approach can recover the residual norm one would get from a successful IR iteration.</p><p>There is also a storage problem. One should allocate storage for the Krylov basis vectors and other vectors that GMRES needs internally. We do that in the factorization phase. So the structure <strong>MPGEFact</strong> has the  factorization of the low precision matrix, the residual, the Krylov basis and some other vectors needed in the solve. </p><p>The Julia function <code>mpglu</code> constructs the data structure and factors the low precision copy of the matrix. The output, like that of <code>mplu</code> is a factorization object that you can use with backslash.</p><p>Here is a well conditioned example. Both IR and GMRES-IR perform well, with GMRES-IR taking significantly more time.  </p><pre><code class="nohighlight hljs">julia&gt; using MultiPrecisionArrays

julia&gt; using MultiPrecisionArrays.Examples

julia&gt; using BenchmarkTools

julia&gt; N=4069; AD= I - Gmat(N); A=Float32.(AD); x=ones(Float32,N); b=A*x;

julia&gt; MPF=mplu(A); MPF2=mpglu(A);

julia&gt; z=MPF\b; y=MPF2\b; println(norm(z-x,Inf),&quot;  &quot;,norm(y-x,Inf))
5.9604645e-7  4.7683716e-7

julia&gt; @btime $MPF\$b;
  13.582 ms (4 allocations: 24.33 KiB)

julia&gt; @btime $MPF2\$b;
  40.344 ms (183 allocations: 90.55 KiB)</code></pre><p>If you dig into the iterations statistics (more on that later) you will see that the GMRES-IR iteration took almost exactly four times as many solves and residual computations as the simple IR solve.</p><p>We will repeat this experiment on the ill-conditioned example. In this example, as we saw earlier, IR fails to converge.</p><pre><code class="nohighlight hljs">julia&gt; N=4069; AD= I - 800.0*Gmat(N); A=Float32.(AD); x=ones(Float32,N); b=A*x;
        
julia&gt; MPF=mplu(A); MPF2=mpglu(A);

julia&gt; z=MPF\b; y=MPF2\b; println(norm(z-x,Inf),&quot;  &quot;,norm(y-x,Inf))
0.2875508  0.004160166

julia&gt; println(norm(b-A*z,Inf)/norm(b,Inf),&quot;  &quot;,norm(b-A*y,Inf)/norm(b,Inf))
0.0012593127  1.4025759e-5</code></pre><p>So, the relative error and relative residual norm for GMRES-IR is much smaller than that for IR.</p><h2 id="BiCGSTAB-IR"><a class="docs-heading-anchor" href="#BiCGSTAB-IR">BiCGSTAB-IR</a><a id="BiCGSTAB-IR-1"></a><a class="docs-heading-anchor-permalink" href="#BiCGSTAB-IR" title="Permalink"></a></h2><p><strong>MultiPrecisionArrays.jl</strong> also supports BiCGSTAB (<a href="../References/#bicgstab">der Vorst, 1992</a>). The API is the same as for GMRES-IR with the swap of <code>b</code> for <code>g</code>. So the data structure is <strong>MPBArray</strong> and the factorizations are <code>mpblu</code> and <code>mpblu!</code>. Keep in mind that a BiCGSTAB iteration costs two matrix-vector and two preconditioner-vector products.</p><p>We will do the same examples we did for GMRES-IR.</p><pre><code class="nohighlight hljs">julia&gt; using MultiPrecisionArrays

julia&gt; using MultiPrecisionArrays.Examples

julia&gt; using BenchmarkTools

julia&gt; N=4069; AD= I - Gmat(N); A=Float32.(AD); x=ones(Float32,N); b=A*x;

julia&gt; MPFB=mpblu(A); MPF=mplu(A);

julia&gt; z=MPF\b; y=MPFB\b; println(norm(z-x,Inf),&quot;  &quot;,norm(y-x,Inf))
5.9604645e-7  4.172325e-7

julia&gt; @btime $MPF\$b;
  13.371 ms (5 allocations: 24.38 KiB)

julia&gt; @btime $MPFB\$b;
  77.605 ms (178 allocations: 821.45 KiB)</code></pre><p>The only new thing is that the solve for BiCGSAB-IR took roughly twice as long. That is no surprise since the cost of a BiCGSTAB iteration is about double that of a GMRES iteration.</p><p>The ill-conditioned example tells the same story.</p><pre><code class="nohighlight hljs">julia&gt; N=4069; AD= I - 800.0*Gmat(N); A=Float32.(AD); x=ones(Float32,N); b=A*x;

julia&gt; MPF=mplu(A); MPFB2=mpblu(A);

julia&gt; z=MPF\b; y=MPFB2\b; println(norm(z-x,Inf),&quot;  &quot;,norm(y-x,Inf))
0.2875508  0.0026364326

julia&gt; println(norm(b-A*z,Inf)/norm(b,Inf),&quot;  &quot;,norm(b-A*y,Inf)/norm(b,Inf))
0.0012593127  7.86059e-6</code></pre><p>So, as was the case with GMRES, BiCGSTAB-IR is a much better solver that IR alone.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« Home</a><a class="docs-footer-nextpage" href="../Details/Termination/">Terminating the while loop »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.8.1 on <span class="colophon-date" title="Tuesday 25 February 2025 18:12">Tuesday 25 February 2025</span>. Using Julia version 1.10.8.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
