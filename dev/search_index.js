var documenterSearchIndex = {"docs":
[{"location":"Details/Interprecision_1/#Interprecision-Transfers:-Part-I","page":"Interprecision Transfers: Part I","title":"Interprecision Transfers: Part I","text":"","category":"section"},{"location":"Details/Interprecision_1/","page":"Interprecision Transfers: Part I","title":"Interprecision Transfers: Part I","text":"The discussion in this section is for the most useful case where high precision is Float64 and low precision is Float32. Things are different if low precision is Float16.","category":"page"},{"location":"Details/Interprecision_1/","page":"Interprecision Transfers: Part I","title":"Interprecision Transfers: Part I","text":"Recall that the default way to use the low precision factorization  is to copy r into low precision, scale it, perform the solve in  low precision, and then reverse the scaling and promote the  correction d. So if AF = lu(A32) is  the factorization object for the low precision factorization, then we compute d via","category":"page"},{"location":"Details/Interprecision_1/","page":"Interprecision Transfers: Part I","title":"Interprecision Transfers: Part I","text":"d = norm(r)* Float64.( AF\\ (Float32.(r / norm(r))))","category":"page"},{"location":"Details/Interprecision_1/","page":"Interprecision Transfers: Part I","title":"Interprecision Transfers: Part I","text":"We will refer to this approach as the low precision solve (LPS).  As we said earlier, if one simply does","category":"page"},{"location":"Details/Interprecision_1/","page":"Interprecision Transfers: Part I","title":"Interprecision Transfers: Part I","text":"d = AF\\r","category":"page"},{"location":"Details/Interprecision_1/","page":"Interprecision Transfers: Part I","title":"Interprecision Transfers: Part I","text":"the elements of the triangular matrices are promoted to double as the solves take place. We will refer to this as a mixed precision solve (MPS). In the table below we report timings from Julia's  BenchmarkTools package for double precision matrix vector multiply (MV64), single precision LU factorization (LU32) and three approaches for using the factors to solve a linear system. HPS is the time for a fully double precision triangular solved and MPS and LPS are the mixed precision solve and the fully low precision solve. IR will use a high precision matrix vector multiply to compute the residual and a solve to compute the correction for each iteration. The low precision factorization is done only once.","category":"page"},{"location":"Details/Interprecision_1/","page":"Interprecision Transfers: Part I","title":"Interprecision Transfers: Part I","text":"In this example A = I + 800 G(N) and we look at several values of N.","category":"page"},{"location":"Details/Interprecision_1/","page":"Interprecision Transfers: Part I","title":"Interprecision Transfers: Part I","text":"    N      MV64       LU32       HPS        MPS        LPS   LU32/MPS\n  512    2.8e-05    7.7e-04    5.0e-05    1.0e-04    2.8e-05 7.8e+00\n 1024    1.1e-04    2.6e-03    1.9e-04    7.7e-04    1.0e-04 3.4e+00\n 2048    6.1e-04    1.4e-02    8.8e-04    3.5e-03    4.0e-04 4.0e+00\n 4096    1.9e-03    8.4e-02    4.7e-03    1.4e-02    2.2e-03 5.8e+00\n 8192    6.9e-03    5.9e-01    1.9e-02    5.9e-02    9.7e-03 9.9e+00","category":"page"},{"location":"Details/Interprecision_1/","page":"Interprecision Transfers: Part I","title":"Interprecision Transfers: Part I","text":"The last column of the table is the ratio of timings for the low precision factorization and the mixed precision solve. Keeping in mind that at least two solves will be needed in IR, the table shows that MPS can be a significant fraction of the cost of the solve for smaller problems and that LPS is at least 4 times less costly. This is a compelling case for using LPS in case considered in this section, where high precision is double and low precision is single, provided the performance of IR is equally good.","category":"page"},{"location":"Details/Interprecision_1/","page":"Interprecision Transfers: Part I","title":"Interprecision Transfers: Part I","text":"If one is solving ma vx = vb for multiple right hand sides, as one would do for nonlinear equations in many cases, then LPS is significantly faster for small and moderately large problems. For example, for N=4096 the cost of MPS is roughly 15 of the low precision LU factorization, so if one does more than 6 solves with the same factorization, the solve cost would be more than the factorization cost. LPS is five times faster and we saw this effect while preparing our our nonlinear solver package SIAMFANL.jl. The situation for IR is similar, but one must consider the cost of the high precision matrix-vector multiply, which is about the same as LPS.","category":"page"},{"location":"Details/Interprecision_1/","page":"Interprecision Transfers: Part I","title":"Interprecision Transfers: Part I","text":"We make LPS the default for IR if high precision is double and low precision is single. This decision is good for desktop computing. If low precision is half, then the LPS vs MPS decision needs more scrutiny.","category":"page"},{"location":"Details/Interprecision_1/","page":"Interprecision Transfers: Part I","title":"Interprecision Transfers: Part I","text":"Since MPS does the triangular solves in high precision, one should expect that the results will be more accurate and that the improved accuracy might enable the IR loop to terminate earlier \\cite{CarsonHigham}. We should be able to see that by timing the IR loop after computing the factorization. One should also verify that the residual norms are equally good.","category":"page"},{"location":"Details/Interprecision_1/","page":"Interprecision Transfers: Part I","title":"Interprecision Transfers: Part I","text":"We will conclude this section with two final tables for the results of IR with A = I + alpha G(N). We compare the well conditioned case (alpha=1) and the ill-conditioned case (alpha=800) for a few values of N. We will look at residual and error norms for both approaches to interprecision transfer. The conclusion is that if high precision is double and low is single, the two approaches give equally good results. ","category":"page"},{"location":"Details/Interprecision_1/","page":"Interprecision Transfers: Part I","title":"Interprecision Transfers: Part I","text":"The columns of the tables are the dimensions, the ell^infty relative error norms for both LP and MP interprecision transfers (ELP and EMP) and the corresponding relative residual norms (RLP and RMP).","category":"page"},{"location":"Details/Interprecision_1/","page":"Interprecision Transfers: Part I","title":"Interprecision Transfers: Part I","text":"The results for alpha=1 took 5 IR iterations for all cases. As expected the LPS iteration was faster than MPS. However, for the ill-conditioned alpha=800 case, MPS took one fewer iteration (5 vs 6) than EPS for all but the smallest problem. Even so, the overall solve times were essentially the same.","category":"page"},{"location":"Details/Interprecision_1/","page":"Interprecision Transfers: Part I","title":"Interprecision Transfers: Part I","text":"alpha=1","category":"page"},{"location":"Details/Interprecision_1/","page":"Interprecision Transfers: Part I","title":"Interprecision Transfers: Part I","text":"    N      ELP        EMP        RLP         RMP        TLP       TMP \n  512    4.4e-16    5.6e-16    3.9e-16    3.9e-16    2.8e-04   3.6e-04 \n 1024    6.7e-16    4.4e-16    3.9e-16    3.9e-16    1.2e-03   1.5e-03 \n 2048    5.6e-16    4.4e-16    3.9e-16    3.9e-16    5.8e-03   6.2e-03 \n 4096    1.1e-15    1.1e-15    7.9e-16    7.9e-16    1.9e-02   2.4e-02 \n 8192    8.9e-16    6.7e-16    7.9e-16    5.9e-16    7.0e-02   8.9e-02 ","category":"page"},{"location":"Details/Interprecision_1/","page":"Interprecision Transfers: Part I","title":"Interprecision Transfers: Part I","text":"alpha=800","category":"page"},{"location":"Details/Interprecision_1/","page":"Interprecision Transfers: Part I","title":"Interprecision Transfers: Part I","text":"    N      ELP        EMP        RLP         RMP        TLP       TMP \n  512    6.3e-13    6.2e-13    2.1e-15    1.8e-15    3.3e-04   3.8e-04 \n 1024    9.6e-13    1.1e-12    3.4e-15    4.8e-15    1.3e-03   1.4e-03 \n 2048    1.0e-12    1.2e-12    5.1e-15    4.5e-15    7.2e-03   6.8e-03 \n 4096    2.1e-12    2.1e-12    6.6e-15    7.5e-15    2.4e-02   2.5e-02 \n 8192    3.3e-12    3.2e-12    9.0e-15    1.0e-14    8.4e-02   8.9e-02 ","category":"page"},{"location":"functions/mpgeslir/#mpgeslir:-IR-solver","page":"mpgeslir: IR solver","title":"mpgeslir: IR solver","text":"","category":"section"},{"location":"functions/mpgeslir/","page":"mpgeslir: IR solver","title":"mpgeslir: IR solver","text":"mpgeslir(AF::MPFact, b; reporting=false, verbose=true)\nmpgeslir(AF::MPArray, b; reporting=false, verbose=true)","category":"page"},{"location":"functions/mpgeslir/#MultiPrecisionArrays.mpgeslir-Tuple{Union{MultiPrecisionArrays.MPHFact, MPLFact}, Any}","page":"mpgeslir: IR solver","title":"MultiPrecisionArrays.mpgeslir","text":"mpgeslir(AF::MPFact, b; TR=Float16, reporting=false, verbose=true)\n\nI do not export this function. The idea is that you use mpglu and do not touch either the constructor or the solver directly.\n\nUse a multi-precision factorization to solve a linear system with plain vanilla iterative refinement.\n\nThis version is analogous to A\\b and combines the factorization and the solve. You start with MPA=MPArray(A) and then pass MPA to mpgeslir and combine the factorization and the solve. \n\nYou can also get the multiprecision factorization directly with\n\nMPF=mplu(A)\n\nand then pass MPF to mpgeslir.\n\nI use this to get some timing results and it's also convenient if you want to do factor and solve in one statement. \n\nYou can also get this with x = MPA\\b.\n\nIf you set the kwarg reporting to true you can get the IR residual history. The output of \n\nx = MPA\\b\n\nUse a multi-precision factorization to solve a linear system with plain vanilla iterative refinement.\n\nMPFact is a union of all the MultiPrecision factorizations in the package.  The triangular solver will dispatch on the various types depending on how the interprecision transfers get done.\n\n\n\n\n\n","category":"method"},{"location":"functions/mpgeslir/#MultiPrecisionArrays.mpgeslir-Tuple{MPArray, Any}","page":"mpgeslir: IR solver","title":"MultiPrecisionArrays.mpgeslir","text":"mpgeslir(MPA::MPArray, b; TR=Float16, reporting = false, verbose = true)\n\nI do not export this function. The idea is that you use mpglu and do not touch either the constructor or the solver directly.\n\nUse a multi-precision factorization to solve a linear system with plain vanilla iterative refinement.\n\nThis version is analogous to A\\b and combines the factorization and the solve. You start with MPA=MPArray(A) and then pass MPA to mpgeslir and combine the factorization and the solve. \n\nYou can also get the multiprecision factorization directly with\n\nMPF=mplu(A)\n\nand then pass MPF to mpgeslir.\n\nI use this to get some timing results and it's also convenient if you want to do factor and solve in one statement. \n\nYou can also get this with x = MPA\\b.\n\nIf you set the kwarg reporting to true you can get the IR residual history. The output of \n\nx = MPA\\b\n\nor\n\nx=MPF\\b\n\nis the solition. The output of \n\nmout = \\(MPA,b; reporting=true)\n\nor\n\nmout = \\(MPF,b; reporting=true)\n\nis a structure. mpout.sol is the solution. mpout.rhist is the residual history. mpout also contains the datatypes TW for high precision and TF for low precision.\n\nExample\n\njulia> using MultiPrecisionArrays.Examples\n\njulia> N=4096; A = I - 800.0 * Gmat(N); b=ones(N);\n\njulia> MPF=mplu(A);\n\njulia> mout=\\(MPF, b; reporting=true);\n\njulia> mout.rhist\n6-element Vector{Float64}:\n 1.00000e+00\n 5.36483e-02\n 1.57977e-05\n 5.10232e-09\n 7.76756e-12\n 9.90008e-12\n\n# Stagnation after four IR iterations\n\njulia> [mout.TW mout.TF]\n1×2 Matrix{DataType}:\n Float64  Float32\n\n\nThe TR kwarg is the residual precision. Leave this alone unless you know what you are doing. TR is a higher precision than TW and when you set TR you are essentially solving TR.(A) x = TR.(b)  with IR with the factorization in TF and the residual computation done via A TR.(x) and  interprecision transfers on the fly. So, the storage cost is the matrix, and the copy in the factoriztion precision.\n\nThe classic case is TW = TF = Float32 and TR = Float64. The nasty part of this is that you must store TWO copies of the matrix. One for the residual computation and the other to overwrite with the factors. I do not think this is a good deal. My support for this through mplu. To do this you must put the  TR kwarg explicitly in your call to the solver.\n\nExample\n\njulia> using MultiPrecisionArrays.Examples\n\njulia> n=31; alpha=Float32(1.0);\n\njulia> G=Gmat(n, Float32);\n\njulia> A = I + alpha*G;\n\njulia> b = A*ones(Float32,n);\n\n# use mpa with TF=TW=Float32\n\nAF = mplu(A; TF=Float32, onthefly=true);\n\n# now set TR=Float64 with the kwarg and solve\n\nmout = \\(AF, b; TR=Float64, reporting=true);\n\n# The solution and the residuals are in double. The iteration drives\n# the residual (evaluated in double) to close to double precision roundoff\n\njulia> mout.rhist\n4-element Vector{Float64}:\n 1.12500e+00\n 2.65153e-07\n 6.90559e-14\n 6.66134e-16\n\n# What does this mean. I'll solve the promoted problem. TR.(A) x = b\n\njulia> AD=Float64.(A);\n\njulia> xd = AD\\b;\n\njulia> norm(xd - mout.sol,Inf)\n1.11022e-15\n\n# Is that better?\n\n\n\n\n\n","category":"method"},{"location":"functions/mpkrir/#mpkrir:-Krylov-IR-solver","page":"mpkrir: Krylov-IR solver","title":"mpkrir: Krylov-IR solver","text":"","category":"section"},{"location":"functions/mpkrir/","page":"mpkrir: Krylov-IR solver","title":"mpkrir: Krylov-IR solver","text":"mpkrir(AF::MPKFact, b; reporting = false, verbose = false, mpdebug = false)","category":"page"},{"location":"functions/mpkrir/#MultiPrecisionArrays.mpkrir-Tuple{Union{MPBFact, MPGEFact, MultiPrecisionArrays.MPGHFact}, Any}","page":"mpkrir: Krylov-IR solver","title":"MultiPrecisionArrays.mpkrir","text":"mpkrir(AF::MPKFact, b; reporting=false, verbose=false, mpdebug=false)\n\nI do not export this function. The idea is that you use mpglu and do not touch either the constructor or the solver directly.\n\nUse a multi-precision factorization to solve a linear system with plain vanilla iterative refinement.\n\nThis version is analogous to A\\b and combines the factorization and the solve. You start with MPA=MPArray(A) and then pass MPA to mpgeslir and combine the factorization and the solve. \n\nYou can also get the multiprecision factorization directly with\n\nMPF=mplu(A)\n\nand then pass MPF to mpgeslir.\n\nI use this to get some timing results and it's also convenient if you want to do factor and solve in one statement. \n\nYou can also get this with x = MPA\\b.\n\nIf you set the kwarg reporting to true you can get the IR residual history. The output of \n\nx = MPA\\b\n\nKrylov-IR solver \n\nThis is the generic solver used by GMRES-IR and BiCGSTAB-IR. You use the correct MPKFact = Union{MPGFact,MPBFact} structure and mpkrir will do the right thing. \n\nYou should not be calling this directly. Use \\ to solve linear systems with the multiprecision factorization and to  use the optional kwargs.\n\nWe overload the backslash operator to call mpkrir for a multiprecision MPGFact factorization. So if MPA is an MPGArray and  \n\nAF = mpglu!(MPA)\n\nThen AF\\b maps to\n\nmpkrir(AF, b)\n\nwhich does the GMRES-IR solve. You can also get the multiprecision factoriztion directly with\n\nAF = mpglu(A)\n\nwhich builds the mutliprcision MPGArray and then factors the low preicsion copy.\n\nSimilarly if  MPA is an MPBArray. Then\n\nAF = mpblu!(MPA)\n\nThen AF\\b maps to\n\nmpkrir(AF, b)\n\nwhich does the BiCGSTAB-IR solve.\n\nYou can also use the \\ operator to harvest iteration statistics.\n\nExample\n\n```jldoctest julia> using MultiPrecisionArrays.Examples\n\njulia> N=4096; A = I - 800.0 * Gmat(N); b=ones(N);\n\njulia> AF=mpglu(A);\n\njulia> solout=(AF, b; reporting=true);\n\nCorrect result?\n\njulia> x=solout.sol; norm(b-A*x,Inf) 9.12193e-12\n\nLook at the residual history\n\njulia> solout.rhist 5-element Vector{Float64}:  1.00000e+00  1.16784e-10  8.47566e-12  7.95053e-12  9.12193e-12\n\nStagnation after the 3rd iteration. Now the Krylovs/iteration\n\njulia> solout.khist 4-element Vector{Int64}:  4  5  4  5\n\n4-5 Krylovs per iteration.\n\nBiCGSTAB works the same way.\n\n\n\n\n\n","category":"method"},{"location":"Details/Stats/#Harvesting-Iteration-Statistics","page":"Harvesting Iteration Statistics","title":"Harvesting Iteration Statistics","text":"","category":"section"},{"location":"Details/Stats/","page":"Harvesting Iteration Statistics","title":"Harvesting Iteration Statistics","text":"You can get some iteration statistics by using the reporting keyword argument to the solvers. The easiest way to do this is with the backslash command. When you use this option you get a data structure with the solution and the residual history.","category":"page"},{"location":"Details/Stats/","page":"Harvesting Iteration Statistics","title":"Harvesting Iteration Statistics","text":"julia> using MultiPrecisionArrays\n\njulia> using MultiPrecisionArrays.Examples\n\njulia> N=4096; A = I - Gmat(N); x=ones(N); b=A*x; \n\njulia> MPF=mplu(A);\n\njulia> # Use \\ with reporting=true\n\njulia> mpout=\\(MPF, b; reporting=true);\n\njulia> norm(b-A*mpout.sol, Inf)\n1.33227e-15\n\njulia> # Now look at the residual history\n\njulia> mpout.rhist\n5-element Vector{Float64}:\n 9.99878e-01\n 1.21892e-04\n 5.25805e-11\n 2.56462e-14\n 1.33227e-15","category":"page"},{"location":"Details/Stats/","page":"Harvesting Iteration Statistics","title":"Harvesting Iteration Statistics","text":"As you can see, IR does well for this problem. The package uses an initial iterate of x = 0 and so the initial residual is simply r = b and the first entry in the residual history is  b _infty. The iteration terminates successfully after four matrix-vector products.","category":"page"},{"location":"Details/Stats/","page":"Harvesting Iteration Statistics","title":"Harvesting Iteration Statistics","text":"You may wonder why the residual after the first iteration was so much larger than single precision roundoff. The reason is that the default  when the low precision is single is to downcast the residual to single before  the solve (onthefly=false). One can enable interprecision transfers on the fly and see the difference.","category":"page"},{"location":"Details/Stats/","page":"Harvesting Iteration Statistics","title":"Harvesting Iteration Statistics","text":"julia> MPF2=mplu(A; onthefly=true);\n\njulia> mpout2=\\(MPF2, b; reporting=true);\n\njulia> mpout2.rhist\n5-element Vector{Float64}:\n 9.99878e-01\n 6.17721e-07\n 3.84581e-13\n 7.99361e-15\n 8.88178e-16","category":"page"},{"location":"Details/Stats/","page":"Harvesting Iteration Statistics","title":"Harvesting Iteration Statistics","text":"So the second iteration is much better, but the iteration terminated after four iterations in both cases.","category":"page"},{"location":"Details/Stats/","page":"Harvesting Iteration Statistics","title":"Harvesting Iteration Statistics","text":"There are more examples for this in [2].","category":"page"},{"location":"functions/hlu!/#hlu!:-Get-LU-to-perform-reasonably-well-for-Float16","page":"hlu!: Get LU to perform reasonably well for Float16","title":"hlu!: Get LU to perform reasonably well for Float16","text":"","category":"section"},{"location":"functions/hlu!/","page":"hlu!: Get LU to perform reasonably well for Float16","title":"hlu!: Get LU to perform reasonably well for Float16","text":"hlu!(A::Matrix{T}) where{T}","category":"page"},{"location":"functions/hlu!/#MultiPrecisionArrays.hlu!-Union{Tuple{Matrix{T}}, Tuple{T}} where T","page":"hlu!: Get LU to perform reasonably well for Float16","title":"MultiPrecisionArrays.hlu!","text":"hlu!(A::AbstractMatrix{T}) where {T} Return LU factorization of A\n\nC. T. Kelley, 2023\n\nThis function is a hack of generic_lufact! which is part of\n\nhttps://github.com/JuliaLang/julia/blob/master/stdlib/LinearAlgebra/src/lu.jl\n\nI \"fixed\" the code to be Float16 only and fixed pivoting to only MaxRow.\n\nAll I did in the factorization was thread the critical loop with FLoops.@floop and put @simd in the inner loop. These changes got me a 10x speedup on my Mac M2 Pro with 8 performance cores. I'm happy.\n\n\n\n\n\n","category":"method"},{"location":"Half_1/#Half-Precision-and-Krylov-IR","page":"Half Precision and Krylov-IR","title":"Half Precision and Krylov-IR","text":"","category":"section"},{"location":"Half_1/","page":"Half Precision and Krylov-IR","title":"Half Precision and Krylov-IR","text":"There are two half precision (16 bit) formats. Julia has native support for IEEE 16 bit floats (Float16). A second format (BFloat16) has a larger exponent field and a smaller  significand (mantissa), thereby trading precision for range. In fact, the exponent field in BFloat is the same size (8 bits) as that for single precision (Float32). The  significand, however, is only 8 bits. Which is less than that for  Float16 (11 bits) and single (24 bits). The size of the significand means that you can get in real trouble with half precision in either format.  ","category":"page"},{"location":"Half_1/","page":"Half Precision and Krylov-IR","title":"Half Precision and Krylov-IR","text":"At this point Julia has no native support for BFloat16. Progress is being made and the package BFLoat16s will let you experiment.","category":"page"},{"location":"Half_1/","page":"Half Precision and Krylov-IR","title":"Half Precision and Krylov-IR","text":"Using half precision will not speed anything up, in fact it will make  the solver slower. The reason for this is that LAPACK and the BLAS  do not (YET) support half precision, so all the clever stuff in there is missing. We provide a half precision LU factorization for Float16  /src/Factorizations/hlu!.jl that is better than nothing.  It's a hack of Julia's  generic_lu! with threading and a couple compiler directives. Even so, it's 2.5 – 5 x slower than a  double precision LU. Half precision support for both Float16 and BFloat16 is coming  (Julia and Apple support Float16 in hardware! Apple hardware supports BFloat16). However, for now, at least for desktop computing, half precision is for research in iterative refinement, not applications. ","category":"page"},{"location":"Half_1/","page":"Half Precision and Krylov-IR","title":"Half Precision and Krylov-IR","text":"Here's a table (created with  /CodeForDocs/HalfTime.jl ) that illustrates the point. In the table we compare timings for LAPACK's LU to the LU we compute with hlu!.jl. The matrix is  I-8000*G.","category":"page"},{"location":"Half_1/","page":"Half Precision and Krylov-IR","title":"Half Precision and Krylov-IR","text":"      N       F64       F32       F16     F16/F64 \n     1024  3.65e-03  2.65e-03  5.26e-03  1.44e+00 \n     2048  2.26e-02  1.41e-02  3.70e-02  1.64e+00 \n     4096  1.55e-01  8.53e-02  2.55e-01  1.65e+00 \n     8192  1.15e+00  6.05e-01  4.23e+00  3.69e+00 ","category":"page"},{"location":"Half_1/","page":"Half Precision and Krylov-IR","title":"Half Precision and Krylov-IR","text":"The columns of the table are the dimension of the problem, timings for double, single, and half precision, and the ratio of the half precision timings to double. The timings came from Julia 1.10-beta2 running on an Apple M2 Pro with 8 performance cores.","category":"page"},{"location":"Half_1/#Half-Precision-is-Subtle","page":"Half Precision and Krylov-IR","title":"Half Precision is Subtle","text":"","category":"section"},{"location":"Half_1/","page":"Half Precision and Krylov-IR","title":"Half Precision and Krylov-IR","text":"Half precision is also difficult to use properly. The low precision can  make iterative refinement fail because the half precision factorization  can have a large error. Here is an example to illustrate this point.  The matrix here is modestly ill-conditioned and you can see that in the  error from a direct solve in double precision.","category":"page"},{"location":"Half_1/","page":"Half Precision and Krylov-IR","title":"Half Precision and Krylov-IR","text":"julia> A=I - 800.0*G;\n\njulia> x=ones(N);\n\njulia> b=A*x;\n\njulia> xd=A\\b;\n\njulia> norm(b-A*xd,Inf)\n6.96332e-13\n\njulia> norm(xd-x,Inf)\n2.30371e-12","category":"page"},{"location":"Half_1/","page":"Half Precision and Krylov-IR","title":"Half Precision and Krylov-IR","text":"Now, if we downcast things to half precision, nothing good happens.","category":"page"},{"location":"Half_1/","page":"Half Precision and Krylov-IR","title":"Half Precision and Krylov-IR","text":"julia> AH=Float16.(A);\n\njulia> AHF=hlu!(AH);\n\njulia> z=AHF\\b;\n\njulia> norm(b-A*z,Inf)\n6.25650e-01\n\njulia> norm(z-xd,Inf)\n2.34975e-01","category":"page"},{"location":"Half_1/","page":"Half Precision and Krylov-IR","title":"Half Precision and Krylov-IR","text":"So you get very poor, but unsurprising, results. While MultiPrecisionArrays.jl supports half precision and I use it all the time, it is not something you would use in your own work without looking at the literature and making certain you are prepared for strange results. Getting good results consistently from half precision is an active research area.","category":"page"},{"location":"Half_1/","page":"Half Precision and Krylov-IR","title":"Half Precision and Krylov-IR","text":"So, it should not be a surprise that IR also struggles with half precision. We will illustrate this with one simple example. In this example high precision will be single and low will be half. Using {\\bf MPArray} with a single precision matrix will automatically make the low precision matrix half precision.","category":"page"},{"location":"Half_1/","page":"Half Precision and Krylov-IR","title":"Half Precision and Krylov-IR","text":"julia> N=4096; G=800.0*Gmat(N); A=I - Float32.(G);\n\njulia> x=ones(Float32,N); b=A*x;\n\njulia> MPF=mplu(A; onthefly=false);\n\njulia> y=MPF\\b;\n\njulia> norm(b - A*y,Inf)\n1.05272e+02","category":"page"},{"location":"Half_1/","page":"Half Precision and Krylov-IR","title":"Half Precision and Krylov-IR","text":"So, IR completely failed for this example. We will show how to extract the details of the iteration in a later section.","category":"page"},{"location":"Half_1/","page":"Half Precision and Krylov-IR","title":"Half Precision and Krylov-IR","text":"It is also worthwhile to see if doing the triangular solves on-the-fly (MPS) helps. ","category":"page"},{"location":"Half_1/","page":"Half Precision and Krylov-IR","title":"Half Precision and Krylov-IR","text":"julia> MPBF=mplu(A);\n\njulia> z=MPBF\\b;\n\njulia> norm(b-A*z,Inf)\n1.28174e-03","category":"page"},{"location":"Half_1/","page":"Half Precision and Krylov-IR","title":"Half Precision and Krylov-IR","text":"So, MPS is better in the half precision case. Moreover, it is also less costly thanks to the limited support for half precision computing. For that reason, MPS is the default when high precision is single.","category":"page"},{"location":"Half_1/","page":"Half Precision and Krylov-IR","title":"Half Precision and Krylov-IR","text":"However, on-the-fly solves are not enough to get good results and IR still terminates too soon.","category":"page"},{"location":"Half_1/#GMRES-IR","page":"Half Precision and Krylov-IR","title":"GMRES-IR","text":"","category":"section"},{"location":"Half_1/","page":"Half Precision and Krylov-IR","title":"Half Precision and Krylov-IR","text":"GMRES-IR solves the correction equation with a preconditioned GMRES [9] iteration. One way to think of this is that the solve in the IR loop is an approximate solver for the correction equation","category":"page"},{"location":"Half_1/","page":"Half Precision and Krylov-IR","title":"Half Precision and Krylov-IR","text":"A d = r","category":"page"},{"location":"Half_1/","page":"Half Precision and Krylov-IR","title":"Half Precision and Krylov-IR","text":"where one replaces A with the low precision factors LU. In GMRES-IR one solves the correction equation with a left-preconditioned GMRES iteration using U^-1 L^-1 as the preconditioner. The preconditioned equation is","category":"page"},{"location":"Half_1/","page":"Half Precision and Krylov-IR","title":"Half Precision and Krylov-IR","text":"U^-1 L^-1  A d = U^-1 L^-1 r","category":"page"},{"location":"Half_1/","page":"Half Precision and Krylov-IR","title":"Half Precision and Krylov-IR","text":"GMRES-IR will not be as efficient as IR because each iteration is itself an GMRES iteration and application of the preconditioned matrix-vector product has the same cost (solve + high precision matrix vector product) as a single IR iteration. However, if low precision is half, this approach can recover the residual norm one would get from a successful IR iteration.","category":"page"},{"location":"Half_1/","page":"Half Precision and Krylov-IR","title":"Half Precision and Krylov-IR","text":"There is also a storage problem. One should allocate storage for the Krylov basis vectors and other vectors that GMRES needs internally. We do that in the factorization phase. So the structure MPGEFact has the  factorization of the low precision matrix, the residual, the Krylov basis and some other vectors needed in the solve. ","category":"page"},{"location":"Half_1/","page":"Half Precision and Krylov-IR","title":"Half Precision and Krylov-IR","text":"The Julia function mpglu constructs the data structure and factors the low precision copy of the matrix. The output, like that of mplu is a factorization object that you can use with backslash.","category":"page"},{"location":"Half_1/","page":"Half Precision and Krylov-IR","title":"Half Precision and Krylov-IR","text":"Here is a well conditioned example. Both IR and GMRES-IR perform well, with GMRES-IR taking significantly more time.  ","category":"page"},{"location":"Half_1/","page":"Half Precision and Krylov-IR","title":"Half Precision and Krylov-IR","text":"julia> using MultiPrecisionArrays\n\njulia> using MultiPrecisionArrays.Examples\n\njulia> using BenchmarkTools\n\njulia> N=4069; AD= I - Gmat(N); A=Float32.(AD); x=ones(Float32,N); b=A*x;\n\njulia> MPF=mplu(A); MPF2=mpglu(A);\n\njulia> z=MPF\\b; y=MPF2\\b; println(norm(z-x,Inf),\"  \",norm(y-x,Inf))\n5.9604645e-7  4.7683716e-7\n\njulia> @btime $MPF\\$b;\n  13.582 ms (4 allocations: 24.33 KiB)\n\njulia> @btime $MPF2\\$b;\n  40.344 ms (183 allocations: 90.55 KiB)","category":"page"},{"location":"Half_1/","page":"Half Precision and Krylov-IR","title":"Half Precision and Krylov-IR","text":"If you dig into the iterations statistics (more on that later) you will see that the GMRES-IR iteration took almost exactly four times as many solves and residual computations as the simple IR solve.","category":"page"},{"location":"Half_1/","page":"Half Precision and Krylov-IR","title":"Half Precision and Krylov-IR","text":"We will repeat this experiment on the ill-conditioned example. In this example, as we saw earlier, IR fails to converge.","category":"page"},{"location":"Half_1/","page":"Half Precision and Krylov-IR","title":"Half Precision and Krylov-IR","text":"julia> N=4069; AD= I - 800.0*Gmat(N); A=Float32.(AD); x=ones(Float32,N); b=A*x;\n        \njulia> MPF=mplu(A); MPF2=mpglu(A);\n\njulia> z=MPF\\b; y=MPF2\\b; println(norm(z-x,Inf),\"  \",norm(y-x,Inf))\n0.2875508  0.004160166\n\njulia> println(norm(b-A*z,Inf)/norm(b,Inf),\"  \",norm(b-A*y,Inf)/norm(b,Inf))\n0.0012593127  1.4025759e-5","category":"page"},{"location":"Half_1/","page":"Half Precision and Krylov-IR","title":"Half Precision and Krylov-IR","text":"So, the relative error and relative residual norm for GMRES-IR is much smaller than that for IR.","category":"page"},{"location":"Half_1/#BiCGSTAB-IR","page":"Half Precision and Krylov-IR","title":"BiCGSTAB-IR","text":"","category":"section"},{"location":"Half_1/","page":"Half Precision and Krylov-IR","title":"Half Precision and Krylov-IR","text":"MultiPrecisionArrays.jl also supports BiCGSTAB [10]. The API is the same as for GMRES-IR with the swap of b for g. So the data structure is MPBArray and the factorizations are mpblu and mpblu!. Keep in mind that a BiCGSTAB iteration costs two matrix-vector and two preconditioner-vector products.","category":"page"},{"location":"Half_1/","page":"Half Precision and Krylov-IR","title":"Half Precision and Krylov-IR","text":"We will do the same examples we did for GMRES-IR.","category":"page"},{"location":"Half_1/","page":"Half Precision and Krylov-IR","title":"Half Precision and Krylov-IR","text":"julia> using MultiPrecisionArrays\n\njulia> using MultiPrecisionArrays.Examples\n\njulia> using BenchmarkTools\n\njulia> N=4069; AD= I - Gmat(N); A=Float32.(AD); x=ones(Float32,N); b=A*x;\n\njulia> MPFB=mpblu(A); MPF=mplu(A);\n\njulia> z=MPF\\b; y=MPFB\\b; println(norm(z-x,Inf),\"  \",norm(y-x,Inf))\n5.9604645e-7  4.172325e-7\n\njulia> @btime $MPF\\$b;\n  13.371 ms (5 allocations: 24.38 KiB)\n\njulia> @btime $MPFB\\$b;\n  77.605 ms (178 allocations: 821.45 KiB)","category":"page"},{"location":"Half_1/","page":"Half Precision and Krylov-IR","title":"Half Precision and Krylov-IR","text":"The only new thing is that the solve for BiCGSAB-IR took roughly twice as long. That is no surprise since the cost of a BiCGSTAB iteration is about double that of a GMRES iteration.","category":"page"},{"location":"Half_1/","page":"Half Precision and Krylov-IR","title":"Half Precision and Krylov-IR","text":"The ill-conditioned example tells the same story.","category":"page"},{"location":"Half_1/","page":"Half Precision and Krylov-IR","title":"Half Precision and Krylov-IR","text":"julia> N=4069; AD= I - 800.0*Gmat(N); A=Float32.(AD); x=ones(Float32,N); b=A*x;\n\njulia> MPF=mplu(A); MPFB2=mpblu(A);\n\njulia> z=MPF\\b; y=MPFB2\\b; println(norm(z-x,Inf),\"  \",norm(y-x,Inf))\n0.2875508  0.0026364326\n\njulia> println(norm(b-A*z,Inf)/norm(b,Inf),\"  \",norm(b-A*y,Inf)/norm(b,Inf))\n0.0012593127  7.86059e-6","category":"page"},{"location":"Half_1/","page":"Half Precision and Krylov-IR","title":"Half Precision and Krylov-IR","text":"So, as was the case with GMRES, BiCGSTAB-IR is a much better solver that IR alone.","category":"page"},{"location":"functions/MPArray/#MPArray:-constructor","page":"MPArray: constructor","title":"MPArray: constructor","text":"","category":"section"},{"location":"functions/MPArray/","page":"MPArray: constructor","title":"MPArray: constructor","text":"MPArray(AH::AbstractArray{Float64,2}; TF = Float32, onthefly=false)\nMPArray(AH::AbstractArray{Float32,2}; TF = Float16, onthefly=false)","category":"page"},{"location":"functions/MPArray/#MultiPrecisionArrays.MPArray-Tuple{AbstractMatrix{Float64}}","page":"MPArray: constructor","title":"MultiPrecisionArrays.MPArray","text":"MPArray(AH::AbstractArray{Float64,2}; TF = Float32, onthefly=false) Default constructor for MPArray. \n\nC. T. Kelley 2023\n\nThe MPArray data structure is\n\nstruct MPArray{TW<:AbstractFloat,TF<:AbstractFloat}\n    AH::AbstractArray{TW,2}\n    AL::AbstractArray{TF,2}\n    residual::Vector{TW}\n    onthefly::Bool\nend\n\nThe constructor just builds an MPArray with TW=Float64. Set TF=Float16 to get double/half IR.\n\n\n\n\n\n","category":"method"},{"location":"functions/MPArray/#MultiPrecisionArrays.MPArray-Tuple{AbstractMatrix{Float32}}","page":"MPArray: constructor","title":"MultiPrecisionArrays.MPArray","text":"MPArray(AH::AbstractArray{Float32,2}; TF = Float16, onthefly=true) Default single precision constructor for MPArray with TF=Float16\n\nIf your high precision array is single, then your low precision array is half (Duh!). \n\nWe do the triangular solves with on-the-fly interprecision transfer in this case because the bit of extra accuracy makes a difference and, at least for now, on-the-fly interprecision transfers are cheaper.\n\nData structures etc are the same as in the  double-single/half case, but you don't have the option to go lower than half.\n\n\n\n\n\n","category":"method"},{"location":"functions/mpblu!/#mpblu!:-Factor-a-MPBArray-and-set-it-up-for-BiCGSTAB-by-allocating-room-for-a-few-vectors","page":"mpblu!: Factor a MPBArray and set it up for BiCGSTAB by allocating room for a few vectors","title":"mpblu!: Factor a MPBArray and set it up for BiCGSTAB by allocating room for a few vectors","text":"","category":"section"},{"location":"functions/mpblu!/","page":"mpblu!: Factor a MPBArray and set it up for BiCGSTAB by allocating room for a few vectors","title":"mpblu!: Factor a MPBArray and set it up for BiCGSTAB by allocating room for a few vectors","text":"mpblu!(MPBA::MPBArray)\nmpblu!(MPG::MPBFact, A::AbstractArray{TW,2}) where TW <: Real","category":"page"},{"location":"functions/mpblu!/#MultiPrecisionArrays.mpblu!-Tuple{MPBArray}","page":"mpblu!: Factor a MPBArray and set it up for BiCGSTAB by allocating room for a few vectors","title":"MultiPrecisionArrays.mpblu!","text":"mpblu!(MPBA::MPBArray) Factor a MPBArray and set it up for BiCGSTAB-IR\n\nThis function factors the low precision copy and leaves the high precision matrix alone. The constructor for MPBArray allocates storage for the things BiCGSTAB needs.\n\nYou get a factorization object as output and can use \\ to solve linear systems.\n\n\n\n\n\n","category":"method"},{"location":"functions/mpblu!/#MultiPrecisionArrays.mpblu!-Union{Tuple{TW}, Tuple{MPBFact, AbstractMatrix{TW}}} where TW<:Real","page":"mpblu!: Factor a MPBArray and set it up for BiCGSTAB by allocating room for a few vectors","title":"MultiPrecisionArrays.mpblu!","text":"mpblu!(MPG::MPBFact, A::AbstractArray{TW,2}) where TW <: Real Overwrite a multiprecision factorization MPF to reuse the storage to make a multiprecision factorization of a new matrix A.\n\nThis will, of course, trash the original factorization.\n\nTo use this do\n\nMPG=mpblu!(MPF,A)\n\nSimply using\n\nmpblu!(MPF,A) # Don't do this.\n\n(ie without explicitly returning MPG)\n\nmay not do what you want because the multiprecision factorization structure is immutable and MPF.AF.info cannot be changed.\n\nReassigning MPG works and resuses almost all of the storage in the original array.\n\n\n\n\n\n","category":"method"},{"location":"functions/mpglu/#mpglu:-Combine-MPGArray-construction-and-factorization","page":"mpglu: Combine MPGArray construction and factorization","title":"mpglu: Combine MPGArray construction and factorization","text":"","category":"section"},{"location":"functions/mpglu/","page":"mpglu: Combine MPGArray construction and factorization","title":"mpglu: Combine MPGArray construction and factorization","text":"mpglu(A::AbstractArray{TW,2}; TF=Float32, basissize=10) where TW <: Real","category":"page"},{"location":"functions/mpglu/#MultiPrecisionArrays.mpglu-Union{Tuple{AbstractMatrix{TW}}, Tuple{TW}} where TW<:Real","page":"mpglu: Combine MPGArray construction and factorization","title":"MultiPrecisionArrays.mpglu","text":"mpglu(A::AbstractArray{TW,2}; TF=Float32, basissize=10) where TW <: Real\n\nCombines the constructor of the multiprecision GMRES-ready array with the factorization.\n\nStep 1: build the MPGArray\n\nStep 2: Call mpglu! to build the factorization object\n\n\n\n\n\n","category":"method"},{"location":"functions/mplu!/#mplu!:-Simple-MPArray-factorization","page":"mplu!: Simple MPArray factorization","title":"mplu!: Simple MPArray factorization","text":"","category":"section"},{"location":"functions/mplu!/","page":"mplu!: Simple MPArray factorization","title":"mplu!: Simple MPArray factorization","text":"mplu!(MPA::MPArray)\nmplu!(MPF::MPLFact,A::AbstractArray{TW,2}) where TW <: Real","category":"page"},{"location":"functions/mplu!/#MultiPrecisionArrays.mplu!-Tuple{MPArray}","page":"mplu!: Simple MPArray factorization","title":"MultiPrecisionArrays.mplu!","text":"mplu!(MPA::MPArray)\n\nPlain vanilla MPArray factorization: Factor the low precision copy and leave the high precision matrix alone. You get a factorization object as output and can use \\ to solve linear systems.\n\nThe story on interprecision transfers is that you can set the Boolean onthefly when you construct the MPArray. If you use mplu then you get the defaults\n\nIf onthefly == false then the solver downcasts the residual \n\nbefore the solve and avoids N^2 interprecision transfers.\n\nIf onthefly == true then the solver does interprecision transfers  on the fly and incurs the N^2 interprecision transfer cost for that. \nonthefly == true is what you must use if you plan to use  the low precision  factorization as a preconditioner in IR-GMRES or you're working in  Float16 and the matrix is very ill-conditioned. \nonthefly == nothing means you take the defaults.\n\nIf you want to use static arrays with this stuff, use the  mutable @MArray constructor\n\n\n\n\n\n","category":"method"},{"location":"functions/mplu!/#MultiPrecisionArrays.mplu!-Union{Tuple{TW}, Tuple{MPLFact, AbstractMatrix{TW}}} where TW<:Real","page":"mplu!: Simple MPArray factorization","title":"MultiPrecisionArrays.mplu!","text":"mplu!(MPF::MPLFact,A::AbstractArray{TW,2}) where TW <: Real\n\nOverwrite a multiprecision factorization MPF to reuse the storage to make a multiprecision of a new matrix A.\n\nThis will, of course, trash the original factorization.\n\nTo use this do\n\nMPF=mplu!(MPF,A)\n\nSimply using \n\nmplu!(MPF,A) # Don't do this!\n\n(ie without explicitly returning MPF)\n\nmay not do what you want because the multiprecision factorization structure is immutable and MPF.AF.info cannot be changed.\n\nReassigning MPF works and resuses almost all of the storage in the  original array.\n\nIf you want to use static arrays with this stuff, use the  mutable @MArray constructor\n\n\n\n\n\n","category":"method"},{"location":"functions/MPGArray/#MPGArray:-constructor","page":"MPGArray: constructor","title":"MPGArray: constructor","text":"","category":"section"},{"location":"functions/MPGArray/","page":"MPGArray: constructor","title":"MPGArray: constructor","text":"MPGArray(AH::AbstractArray{Float64,2}; basissize=10, TF=Float32)\nMPGArray(AH::AbstractArray{Float32,2}; basissize=10, TF=Float16)","category":"page"},{"location":"functions/MPGArray/#MultiPrecisionArrays.MPGArray-Tuple{AbstractMatrix{Float64}}","page":"MPGArray: constructor","title":"MultiPrecisionArrays.MPGArray","text":"MPGArray(AH::AbstractArray{Float64,2}; basissize=10, TF=Float32)\n\nAn MPGArray stores the high precision matrix, the low precision factorization, the Krylov basis, and a few other things GMRES needs. If the high precision matrix is double, the low precision is single by default. Half is an optioin which you get with TF=Float16.\n\n\n\n\n\n","category":"method"},{"location":"functions/MPGArray/#MultiPrecisionArrays.MPGArray-Tuple{AbstractMatrix{Float32}}","page":"MPGArray: constructor","title":"MultiPrecisionArrays.MPGArray","text":"MPGArray(AH::AbstractArray{Float32,2}; basissize=10, TF=Float16)\n\nAn MPGArray stores the high precision matrix, the low precision factorization, the Krylov basis, and a few other things GMRES needs. Since High precision is  single, low is half. I'm leaving the kwarg for TF in there because it makes is easier to cut/paste calls to MPGArray different precisions into a CI loop.\n\n\n\n\n\n","category":"method"},{"location":"References/#References","page":"References","title":"References","text":"","category":"section"},{"location":"References/","page":"References","title":"References","text":"C. T. Kelley. MultiPrecisionArrays.jl (2023). Julia Package.\n\n\n\nC. T. Kelley. Using MultiPrecisonArrays.jl: Iterative Refinement in Julia (2023), arXiv:2311.14616 [math.NA].\n\n\n\nC. T. Kelley. SIAMFANLEquations.jl (2022). Julia Package.\n\n\n\nC. T. Kelley. Solving Nonlinear Equations with Iterative Methods: Solvers and Examples in Julia. No. 20 of Fundamentals of Algorithms (SIAM, Philadelphia, 2022).\n\n\n\nC. T. Kelley. Notebook for Solving Nonlinear Equations with Iterative Methods: Solvers and Examples in Julia, https://github.com/ctkelley/NotebookSIAMFANL (2022). IJulia Notebook.\n\n\n\nN. J. Higham. Accuracy and Stability of Numerical Algorithms (Society for Industrial and Applied Mathematics, Philadelphia, PA, USA, 1996); p. xxviii+688.\n\n\n\nJ.J.Dongarra, C.B.Moler and J.H.Wilkinson. Improving the accuracy of computed eigenvalues and eigenvectors. SIAM Journal on Numerical Analysis 20, 23–45 (1983).\n\n\n\nP. Amestoy, A. Buttari, N. Higham, J.-Y. L'Excellent, T. Mary and B. Vieuble. Five-Precision GMRES-based iterative refinement (2023), to appear in SIAM Journal on Matrix Analysis and Applications.\n\n\n\nY. Saad and M. Schultz. GMRES a generalized minimal residual algorithm for solving             nonsymmetric linear systems. SIAM J. Sci. Stat. Comp. 7, 856–869 (1986).\n\n\n\nH. A. der Vorst. Bi-CGSTAB: A fast and smoothly converging variant to Bi-CG for the solution of nonsymmetric systems. SIAM J. Sci. Stat. Comp. 13, 631–644 (1992).\n\n\n\n","category":"page"},{"location":"functions/mpglu!/#mpglu!:-Factor-a-MPGArray-and-set-it-up-for-GMRES-by-allocating-room-for-Krylov-vectors-etc","page":"mpglu!: Factor a MPGArray and set it up for GMRES by allocating room for Krylov vectors etc","title":"mpglu!: Factor a MPGArray and set it up for GMRES by allocating room for Krylov vectors etc","text":"","category":"section"},{"location":"functions/mpglu!/","page":"mpglu!: Factor a MPGArray and set it up for GMRES by allocating room for Krylov vectors etc","title":"mpglu!: Factor a MPGArray and set it up for GMRES by allocating room for Krylov vectors etc","text":"mpglu!(MPGA::MPGArray)\nmpglu!(MPG::MPGEFact, A::AbstractArray{TW,2}) where TW <: Real","category":"page"},{"location":"functions/mpglu!/#MultiPrecisionArrays.mpglu!-Tuple{MPGArray}","page":"mpglu!: Factor a MPGArray and set it up for GMRES by allocating room for Krylov vectors etc","title":"MultiPrecisionArrays.mpglu!","text":"mpglu!(MPGA::MPGArray) Factor a MPGArray using the allocations from the structure.\n\nThis function factors the low precision copy and leaves the high precision matrix alone. The constructor  for MPGArray allocates storage for basissize Kylov vectors and some other things GMRES needs. You get a factorization object as output and can use \\ to solve linear systems.\n\n\n\n\n\n","category":"method"},{"location":"functions/mpglu!/#MultiPrecisionArrays.mpglu!-Union{Tuple{TW}, Tuple{MPGEFact, AbstractMatrix{TW}}} where TW<:Real","page":"mpglu!: Factor a MPGArray and set it up for GMRES by allocating room for Krylov vectors etc","title":"MultiPrecisionArrays.mpglu!","text":"mpglu!(MPG::MPGEFact, A::AbstractArray{TW,2}) where TW <: Real Overwrite a multiprecision factorization MPF to reuse the storage to make a multiprecision of a new matrix A.\n\nThis will, of course, trash the original factorization.\n\nTo use this do\n\nMPG=mpglu!(MPG,A)\n\nSimply using\n\nmpglu!(MPG,A) # Don't do this.\n\n(ie without explicitly returning MPG)\n\nmay not do what you want because the multiprecision factorization structure is immutable and MPF.AF.info cannot be changed.\n\nReassigning MPG works and resuses almost all of the storage in the original array\n\n\n\n\n\n","category":"method"},{"location":"Details/Termination/#Terminating-the-while-loop","page":"Terminating the while loop","title":"Terminating the while loop","text":"","category":"section"},{"location":"Details/Termination/","page":"Terminating the while loop","title":"Terminating the while loop","text":"We terminate the loop when ","category":"page"},{"location":"Details/Termination/","page":"Terminating the while loop","title":"Terminating the while loop","text":" r   tau  b ","category":"page"},{"location":"Details/Termination/","page":"Terminating the while loop","title":"Terminating the while loop","text":"where we use tau = 10 * eps(TW), where eps(TW) is the working precision floating point machine epsilon.  The problem with this criterion is that IR can stagnate, especially for ill-conditioned problems, before the termination criterion is attained. We detect stagnation by looking for a unacceptable decrease (or increase) in the residual norm. So we will terminate the iteration if","category":"page"},{"location":"Details/Termination/","page":"Terminating the while loop","title":"Terminating the while loop","text":" r_new  ge 9  r_old ","category":"page"},{"location":"Details/Termination/","page":"Terminating the while loop","title":"Terminating the while loop","text":"even if the small residual condition is not satisfied.","category":"page"},{"location":"functions/MPBArray/#MPBArray:-constructor","page":"MPBArray: constructor","title":"MPBArray: constructor","text":"","category":"section"},{"location":"functions/MPBArray/","page":"MPBArray: constructor","title":"MPBArray: constructor","text":"MPBArray(AH::AbstractArray{Float64,2}; TF=Float32)\nMPBArray(AH::AbstractArray{Float32,2}; TF=Float16)","category":"page"},{"location":"functions/MPBArray/#MultiPrecisionArrays.MPBArray-Tuple{AbstractMatrix{Float64}}","page":"MPBArray: constructor","title":"MultiPrecisionArrays.MPBArray","text":"MPBArray(AH::AbstractArray{Float64,2}; TF=Float32)\n\nAn MPBArray stores the high precision matrix, the low precision factorization and a few other things BiCGSTAB needs. If the high precision matrix is double, the low precision is single by default. Half is an optioin which you get with TF=Float16.\n\n\n\n\n\n","category":"method"},{"location":"functions/MPBArray/#MultiPrecisionArrays.MPBArray-Tuple{AbstractMatrix{Float32}}","page":"MPBArray: constructor","title":"MultiPrecisionArrays.MPBArray","text":"MPBArray(AH::AbstractArray{Float32,2}; TF=Float16)\n\nAn MPBArray stores the high precision matrix, the low precision factorization and a few other things BiCGSTAB needs. Since High precision is  single, low is half. I'm leaving the kwarg for TF in there because it makes is easier to cut/paste calls to MPBArray different precisions into a CI loop.\n\n\n\n\n\n","category":"method"},{"location":"functions/mpblu/#mpblu:-Combine-MPBArray-construction-and-factorization","page":"mpblu: Combine MPBArray construction and factorization","title":"mpblu: Combine MPBArray construction and factorization","text":"","category":"section"},{"location":"functions/mpblu/","page":"mpblu: Combine MPBArray construction and factorization","title":"mpblu: Combine MPBArray construction and factorization","text":"mpblu(A::AbstractArray{TW,2}; TF=Float32) where TW <: Real","category":"page"},{"location":"functions/mpblu/#MultiPrecisionArrays.mpblu-Union{Tuple{AbstractMatrix{TW}}, Tuple{TW}} where TW<:Real","page":"mpblu: Combine MPBArray construction and factorization","title":"MultiPrecisionArrays.mpblu","text":"mpblu(A::AbstractArray{TW,2}; TF=Float32) where TW <: Real\n\nCombines the constructor of the multiprecision BiCGSTAB-ready array with the factorization.\n\nStep 1: build the MPBArray\n\nStep 2: Call mpblu! to build the factorization object\n\n\n\n\n\n","category":"method"},{"location":"#MultiPrecisionArrays.jl-v0.1.0","page":"Home","title":"MultiPrecisionArrays.jl v0.1.0","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"C. T. Kelley","category":"page"},{"location":"","page":"Home","title":"Home","text":"MultiPrecisionArrays.jl  [1] is a package for iterative refinement. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"These docs are enough to get you started. The complete version with a better account of the theory is [2]. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"This package uses SIAMFANLEquations.jl [3], the solver package associated with a book [4] and suite of IJulia notebooks [5].","category":"page"},{"location":"","page":"Home","title":"Home","text":"This package provides data structures and solvers for several variants of iterative refinement (IR). It will become much more useful when half precision (aka Float16) is fully supported in LAPACK/BLAS. For now, it's only general-purpose application is classical iterative refinement with double precision equations and single precision factorizations.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The half precision stuff is good for those of us doing research in this field. Half precision performance has progressed to the point where you can actually get things done. On an Apple M2-Pro, a half precision LU only costs 3–5 times what a double precision LU costs. This may be as good as it gets unless someone wants to duplicate the LAPACK implementation and get the benefits from blocking, recursion, and clever cache management.","category":"page"},{"location":"","page":"Home","title":"Home","text":"We use a hack-job LU factorization for half precision. Look at the source for hlu!.jl.","category":"page"},{"location":"#What-is-iterative-refinement.","page":"Home","title":"What is iterative refinement.","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The idea is to solve Ax=b in high precision using a factorization in lower precision. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"IR is a perfect example of a storage/time tradeoff. To solve a linear system   Ax=b with IR, one incurs the storage penalty of making a low  precision copy of  and reaps the benefit of only having to factor the  low precision copy.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Here is the textbook  version [6] using the LU factorization.","category":"page"},{"location":"","page":"Home","title":"Home","text":"IR(A, b)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Initialize: x = 0,  r = b\nFactor A = LU in a lower precision\nWhile  r  is too large\nCompute the defect d = (LU)^-1 r\nCorrect the solution x = x + d\nUpdate the residual r = b - Ax\nend","category":"page"},{"location":"","page":"Home","title":"Home","text":"In Julia, a code to do this would solve the linear system A x = b in double precision by using a factorization in a lower precision, say single, within a residual correction iteration. This means that one would need to allocate storage for a copy of A is the lower precision and factor that copy. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"Then one has to determine what the line d = (LU)^-1 r means. Do you cast r into the lower precision before the solve or not? MultiPrecisionArrays.jl provides data structures and solvers to manage this. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"Here's a simple Julia function for IR that","category":"page"},{"location":"","page":"Home","title":"Home","text":"\"\"\"\nIR(A,b)\nSimple minded iterative refinement\nSolve Ax=b\n\"\"\"\nfunction IR(A, b)\n    x = zeros(length(b))\n    r = copy(b)\n    tol = 100.0 * eps(Float64)\n    #\n    # Allocate a single precision copy of A and factor in place\n    #\n    A32 = Float32.(A)\n    AF = lu!(A32)\n    #\n    # Give IR at most ten iterations, which it should not need\n    # in this case\n    #\n    itcount = 0\n    # The while loop will get more attention later.\n    while (norm(r) > tol * norm(b)) && (itcount < 10)\n        #\n        # Store r and d = AF\\r in the same place.\n        #\n        ldiv!(AF, r)\n        x .+= r\n        r .= b - A * x\n        itcount += 1\n    end\n    return x\nend","category":"page"},{"location":"","page":"Home","title":"Home","text":"As written in the function, the defect uses ldiv! to compute AF\\r. This means that the two triangular factors are stored in single precision and interprecision transfers are done with each step in the factorization. While that on the fly interprecision  transfer is an option, and is needed in many situations, the default is to downcast r to low precision, do the solve entirely in low precision, and the upcast the result. The code for that looks like","category":"page"},{"location":"","page":"Home","title":"Home","text":"normr=norm(r)\nds=Float32.(r/normr)\nldiv!(AF, ds)\nr .= Float64.(ds)*normr","category":"page"},{"location":"","page":"Home","title":"Home","text":"The scaling by 1.0/normr helps to avoid underflow, which is most important when the low precision is Float16. We will discuss interprecision  transfer costs later.","category":"page"},{"location":"#Integral-Equations-Example","page":"Home","title":"Integral Equations Example","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The submodule MultiPrecisionArrays.Examples has an example which we will  use for most of the documentation. The function Gmat(N) returns the trapeziod rule discretization of the Greens operator  for -d^2dx^2 on 01 with homogeneous Dirichlet boundary conditions.","category":"page"},{"location":"","page":"Home","title":"Home","text":"G u(x) = int_0^1 g(xy) u(y)  dy ","category":"page"},{"location":"","page":"Home","title":"Home","text":"where","category":"page"},{"location":"","page":"Home","title":"Home","text":"g(xy) = \n    leftbeginarrayc\n        y (1-x)   x  y\n        x (1-y)   x le y\n    endarrayright","category":"page"},{"location":"","page":"Home","title":"Home","text":"The eigenvalues of G are 1(n^2 pi^2) for n = 1 2 dots.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The code for this is in the /src/Examples directory.  The file is Gmat.jl.","category":"page"},{"location":"","page":"Home","title":"Home","text":"In the example we will build a matrix A = I - alpha G. In the examples we will use alpha=10, a very well conditioned case, and alpha=8000 This latter case is very near singularity.","category":"page"},{"location":"","page":"Home","title":"Home","text":"We will solve a linear system with both double precision LU and an MPArray and compare execution time and the quality of the results.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The example below compares the cost of a double precision factorization to a MPArray factorization. The MPArray structure has a high precision and a low precision matrix. The structure we will start with  is","category":"page"},{"location":"","page":"Home","title":"Home","text":"struct MPArray{TW<:AbstractFloat,TF<:AbstractFloat}\n    AH::AbstractArray{TW,2}\n    AL::AbstractArray{TF,2}\n    residual::Vector{TW}\n    onthefly::Bool\nend","category":"page"},{"location":"","page":"Home","title":"Home","text":"The structure also stores the residual. The onthefly Boolean tells the solver how to do the interprecision transfers. The easy way to get started is to use the mplu  command directly on the matrix. That will build the MPArray, follow that with the factorization, and put in all in a structure that you can use with \\.","category":"page"},{"location":"","page":"Home","title":"Home","text":"While we document MPArray and other multiprecision data structures, we do not export the constructors. You should use the multiprecision factorizations instead.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Now we will see how the results look. In this example we compare the result with iterative refinement with A\\b, which is LAPACK's LU.  As you can see the results are equally good. Note that the factorization object MPF is the output of mplu. This is analogous to AF=lu(A) in LAPACK.","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> using MultiPrecisionArrays\n\njulia> using MultiPrecisionArrays.Examples\n\njulia> using BenchmarkTools\n\njulia> N=4096;\n\njulia> G=Gmat(N);\n\njulia> A = I - G;\n\njulia> MPF=mplu(A); AF=lu(A);\n\njulia> z=MPF\\b; w=AF\\b;\n\njulia> ze=norm(z-x,Inf); zr=norm(b-A*z,Inf)/norm(b,Inf);\n\njulia> we=norm(w-x,Inf); wr=norm(b-A*w,Inf)/norm(b,Inf);\n\njulia> println(\"Errors: $ze, $we. Residuals: $zr, $wr\")\nErrors: 8.88178e-16, 7.41629e-14. Residuals: 1.33243e-15, 7.40609e-14\n","category":"page"},{"location":"","page":"Home","title":"Home","text":"So the results are equally good.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The compute time for mplu should be a bit more than half that of lu!. The reason is that mplu factors a low precision array, so the factorization cost is cut in half. Memory is a different story. The reason is that both mplu and lu! do not allocate storage for a new high precision array, but mplu allocates for a low precision copy, so the memory and allocation cost formpluis 50% more thanlu`. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> @belapsed mplu($A)\n8.60945e-02\n\njulia> @belapsed lu!(AC) setup=(AC=copy($A))\n1.42840e-01\n","category":"page"},{"location":"","page":"Home","title":"Home","text":"It is no surprise that the factorization in single precision took roughly half as long as the one in double. In the double-single precision case, iterative refinement is a great example of a time/storage tradeoff. You have to store a low precision copy of A, so the storage burden increases by 50\\% and the factorization time is cut in half. The advantages of IR increase as the dimension increases. IR is less impressive for smaller problems and can even be slower","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> N=30; A=I + Gmat(N); \n\njulia> @belapsed mplu($A)\n4.19643e-06\n\njulia> @belapsed lu!(AC) setup=(AC=copy($A))\n3.70825e-06","category":"page"},{"location":"#Options-and-data-structures-for-mplu","page":"Home","title":"Options and data structures for mplu","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Here is the source for mplu","category":"page"},{"location":"","page":"Home","title":"Home","text":"\"\"\"\nmplu(A::AbstractArray{Float64,2}; TF=Float32, onthefly=false)\n\nCombines the constructor of the multiprecision array with the\nfactorization. \n\"\"\"\nfunction mplu(A::AbstractArray{TW,2}; TF=Float32, onthefly=nothing) where TW <: Real\n#\n# If the high precision matrix is single, the low precision must be half.\n#\n(TW == Float32) && (TF = Float16)\n#\n# Unless you tell me otherwise, onthefly is true if low precision is half\n# and false if low precision is single.\n#\n(onthefly == nothing ) && (onthefly = (TF==Float16))\nMPA=MPArray(A; TF=TF, onthefly=onthefly)\nMPF=mplu!(MPA)\nreturn MPF\nend","category":"page"},{"location":"","page":"Home","title":"Home","text":"The function mplu has two keyword arguments. The easy one to understand is TF which is the precision of the factorization. Julia has support for single (Float32) and half (Float16) precisions. If you set TF=Float16 then low precision will be half. Don't do that unless you know what you're doing. Using half precision is a fast way to get incorrect results. Look at the section on half precision in this Readme for a bit more bad news.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The other keyword arguement is onthefly. That keyword controls how the triangular solvers from the factorization work. When you solve","category":"page"},{"location":"","page":"Home","title":"Home","text":"LU d = r","category":"page"},{"location":"","page":"Home","title":"Home","text":"The LU factors are in low precision and the residual r is in high precision. If you let Julia and LAPACK figure out what to do, then the solves will be done in high precision and the entries in the LU factors will be converted to high precision with each binary operation. The output d will be in high precision. This is called interprecision transfer on-the-fly and onthefly = true will tell the solvers to do it that way. You have N^2 interprecision transfers with each solve and, as we will see, that can have a non-trivial cost.","category":"page"},{"location":"","page":"Home","title":"Home","text":"When low precision is Float32, then the default is (onthefly = false). This converts r to low precision, does the solve entirely in low precision, and then promotes d to high precision. You need to be careful to avoid overflow and, more importantly, underflow when you do that and we scale r to be a unit vector before conversion to low precision and reverse the scaling when we promote d. We take care of this for you.","category":"page"},{"location":"","page":"Home","title":"Home","text":"mplu calls the constructor for the multiprecision array and then factors the low precision matrix. In some cases, such as nonlinear solvers, you will want to separate the constructor and the factorization. When you do that remember that mplu! overwrites the low precision copy of A with the factors. The factorization object is different from the multiprecision array, even though they share storage. This is just like lu!.","category":"page"},{"location":"#Memory-Allocations-for-mplu","page":"Home","title":"Memory Allocations for mplu","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The memory footprint of a multiprecision array is dominated by the high precision array and the low precision copy. The allocations of","category":"page"},{"location":"","page":"Home","title":"Home","text":"AF1=lu(A)","category":"page"},{"location":"","page":"Home","title":"Home","text":"and","category":"page"},{"location":"","page":"Home","title":"Home","text":"AF2=mplu(A)","category":"page"},{"location":"","page":"Home","title":"Home","text":"are very different. Typically lu makes a high precision copy of A  and factors that with lu!. mplu on the other hand, uses A as the high precision matrix in the multiprecision array structure and the makes a low precision copy to send to lu!. Hence mplu has half the allocation burden of lu.","category":"page"},{"location":"","page":"Home","title":"Home","text":"That is, of course misleading. The memory-efficient  way to apply lu is to overwrite A with the factorization using","category":"page"},{"location":"","page":"Home","title":"Home","text":"AF1=lu!(A).","category":"page"},{"location":"","page":"Home","title":"Home","text":"mplu uses an analog of this approach internally. mplu first builds an MPArray structure with","category":"page"},{"location":"","page":"Home","title":"Home","text":"MPA = MPArray(A)","category":"page"},{"location":"","page":"Home","title":"Home","text":"which makes A the high precision matrix and also makes a low precision copy. This is the stage where the extra memory is allocated for the the low precision copy. Then mplu computes the factorization of the low precision matrix with mplu! to construct the factorization object.","category":"page"},{"location":"","page":"Home","title":"Home","text":"MPF = mplu!(MPA).","category":"page"},{"location":"","page":"Home","title":"Home","text":"So, the function mplu simply applies MPArray and follows that  with mplu!. ","category":"page"},{"location":"#Other-IR-software-in-Julia","page":"Home","title":"Other IR software in Julia","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The package IterativeRefinement.jl is an implementation of the IR method from [7].","category":"page"},{"location":"","page":"Home","title":"Home","text":"The unregistered package Itref.jl implements IR and the GMRES-IR method from  [8] and was used to obtain the numerical results in that paper. It does not provide the data structures for preallocation that we do and does not seem to have been updated lately.","category":"page"},{"location":"functions/mplu/#mplu:-Combine-MPArray-construction-and-factorization","page":"mplu: Combine MPArray construction and factorization","title":"mplu: Combine MPArray construction and factorization","text":"","category":"section"},{"location":"functions/mplu/","page":"mplu: Combine MPArray construction and factorization","title":"mplu: Combine MPArray construction and factorization","text":"mplu(A::AbstractArray{TW,2}; TF=Float32, onthefly=nothing) where TW <: Real","category":"page"},{"location":"functions/mplu/#MultiPrecisionArrays.mplu-Union{Tuple{AbstractMatrix{TW}}, Tuple{TW}} where TW<:Real","page":"mplu: Combine MPArray construction and factorization","title":"MultiPrecisionArrays.mplu","text":"mplu(A::AbstractArray{TW,2}; TF=nothing, onthefly=nothing) where TW <: Real\n\nCombines the constructor of the multiprecision array with the factorization. \n\nStep 1: build the MPArray \n\nStep 2: factor the low precision copy and return the factorization object\n\n\n\n\n\n","category":"method"}]
}
