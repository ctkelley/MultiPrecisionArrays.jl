<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Home · MultiPrecisionArrays.jl</title><meta name="title" content="Home · MultiPrecisionArrays.jl"/><meta property="og:title" content="Home · MultiPrecisionArrays.jl"/><meta property="twitter:title" content="Home · MultiPrecisionArrays.jl"/><meta name="description" content="Documentation for MultiPrecisionArrays.jl."/><meta property="og:description" content="Documentation for MultiPrecisionArrays.jl."/><meta property="twitter:description" content="Documentation for MultiPrecisionArrays.jl."/><script data-outdated-warner src="assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="assets/documenter.js"></script><script src="search_index.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href>MultiPrecisionArrays.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li class="is-active"><a class="tocitem" href>Home</a><ul class="internal"><li><a class="tocitem" href="#What-is-iterative-refinement."><span>What is iterative refinement.</span></a></li><li><a class="tocitem" href="#Integral-Equations-Example"><span>Integral Equations Example</span></a></li><li><a class="tocitem" href="#Options-and-data-structures-for-mplu"><span>Options and data structures for mplu</span></a></li><li><a class="tocitem" href="#Memory-Allocations-for-mplu"><span>Memory Allocations for mplu</span></a></li><li><a class="tocitem" href="#Other-IR-software-in-Julia"><span>Other IR software in Julia</span></a></li></ul></li><li><span class="tocitem">Half Precision and Krylov-IR</span><ul><li><a class="tocitem" href="Half_1/">Half Precision and Krylov-IR</a></li></ul></li><li><span class="tocitem">More than you want to know</span><ul><li><a class="tocitem" href="Details/Termination/">Terminating the while loop</a></li><li><a class="tocitem" href="Details/N2Work/">Is O(N^2) work negligible?</a></li><li><a class="tocitem" href="Details/Interprecision_1/">Interprecision Transfers: Part I</a></li><li><a class="tocitem" href="Details/Extended/">Evaluating residuals in higher precision</a></li></ul></li><li><span class="tocitem">MPArray Constructors</span><ul><li><a class="tocitem" href="functions/MPArray/">MPArray: constructor</a></li><li><a class="tocitem" href="functions/MPGArray/">MPGArray: constructor</a></li><li><a class="tocitem" href="functions/MPBArray/">MPBArray: constructor</a></li></ul></li><li><span class="tocitem">Factorizations</span><ul><li><a class="tocitem" href="functions/hlu!/">hlu!: Get LU to perform reasonably well for Float16</a></li><li><a class="tocitem" href="functions/mplu!/">mplu!: Simple MPArray factorization</a></li><li><a class="tocitem" href="functions/mplu/">mplu: Combine MPArray construction and factorization</a></li><li><a class="tocitem" href="functions/mpglu!/">mpglu!: Factor a MPGArray and set it up for GMRES by allocating room for Krylov vectors etc</a></li><li><a class="tocitem" href="functions/mpglu/">mpglu: Combine MPGArray construction and factorization</a></li><li><a class="tocitem" href="functions/mpblu!/">mpblu!: Factor a MPBArray and set it up for BiCGSTAB by allocating room for a few vectors</a></li><li><a class="tocitem" href="functions/mpblu/">mpblu: Combine MPBArray construction and factorization</a></li></ul></li><li><span class="tocitem">Iteration Statistics</span><ul><li><a class="tocitem" href="Details/Stats/">Harvesting Iteration Statistics</a></li></ul></li><li><span class="tocitem">Solvers</span><ul><li><a class="tocitem" href="functions/mpgeslir/">mpgeslir: IR solver</a></li><li><a class="tocitem" href="functions/mpkrir/">mpkrir: Krylov-IR solver</a></li></ul></li><li><span class="tocitem">Termination</span><ul><li><a class="tocitem" href="functions/update_parms/">update_parms: Adjust termination criteria for while loop in IR</a></li></ul></li><li><span class="tocitem">References</span><ul><li><a class="tocitem" href="References/">References</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Home</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Home</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/ctkelley/MultiPrecisionArrays.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/ctkelley/MultiPrecisionArrays.jl/blob/main/docs/src/index.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="multiprecisionarrays.jl-v0.1.4"><a class="docs-heading-anchor" href="#multiprecisionarrays.jl-v0.1.4">multiprecisionarrays.jl v0.1.4</a><a id="multiprecisionarrays.jl-v0.1.4-1"></a><a class="docs-heading-anchor-permalink" href="#multiprecisionarrays.jl-v0.1.4" title="Permalink"></a></h1><p><a href="https://ctkelley.github.io">C. T. Kelley</a></p><p><a href="https://github.com/ctkelley/MultiPrecisionArrays.jl">MultiPrecisionArrays.jl</a>  (<a href="References/#ctk_mparrays">Kelley, 2024</a>) is a package for iterative refinement. </p><p>These docs are enough to get you started. The complete version with a better account of the theory is (<a href="References/#ctk_mparraysdocs">Kelley, 2024</a>). </p><p>This package uses <strong>SIAMFANLEquations.jl</strong> (<a href="References/#ctk_siamfanl">Kelley, 2022</a>), the solver package associated with a book (<a href="References/#ctk_fajulia">Kelley, 2022</a>) and suite of IJulia notebooks (<a href="References/#ctk_notebooknl">Kelley, 2022</a>).</p><p>This package provides data structures and solvers for several variants of iterative refinement (IR). It will become much more useful when half precision (aka <code>Float16</code>) is fully supported in LAPACK/BLAS. For now, it&#39;s only general-purpose application is classical iterative refinement with double precision equations and single precision factorizations.</p><p>The half precision stuff is good for those of us doing research in this field. Half precision performance has progressed to the point where you can actually get things done. On an Apple M2-Pro, a half precision LU only costs 3–5 times what a double precision LU costs. This may be as good as it gets unless someone wants to duplicate the LAPACK implementation and get the benefits from blocking, recursion, and clever cache management.</p><p>We use a hack-job LU factorization for half precision. Look at the source for <strong>hlu!.jl</strong>.</p><h2 id="What-is-iterative-refinement."><a class="docs-heading-anchor" href="#What-is-iterative-refinement.">What is iterative refinement.</a><a id="What-is-iterative-refinement.-1"></a><a class="docs-heading-anchor-permalink" href="#What-is-iterative-refinement." title="Permalink"></a></h2><p>The idea is to solve <span>$Ax=b$</span> in high precision (the <strong>working precision</strong>) <code>TW</code> with a factorization in lower precision (the <strong>factorization precision</strong>) <code>TF</code> and a residual computed in precision (the <strong>residual precision</strong>) <code>TR</code>.</p><p>In the case where the working precision and the residual precision are the same, one can view IR as  a perfect example of a storage/time tradeoff. To solve a linear system   <span>$Ax=b$</span> with IR, one incurs the storage penalty of making a low  precision copy of  and reaps the benefit of only having to factor the  low precision copy.</p><p>IR is also used to address very poor conitioning. IR was invented (<a href="References/#Wilkinson48">Wilkinson, 1948</a>) with this in mind. Here one stores and factors <span>$A$</span> in one precision but evaluates the residual in a higher precision.</p><p>Here is the textbook  version (<a href="References/#higham">Higham, 1996</a>) using the LU factorization.</p><p><strong>IR(A, b)</strong></p><ul><li>Initialize: <span>$x = 0$</span>,  <span>$r = b$</span></li><li>Factor <span>$A = LU$</span> in a lower precision</li><li>While <span>$\| r \|$</span> is too large<ul><li>Compute the defect (correction) <span>$d = (LU)^{-1} r$</span></li><li>Correct the solution <span>$x = x + d$</span></li><li>Update the residual <span>$r = b - Ax$</span></li></ul></li><li>end</li></ul><p>In Julia, a code to do this would solve the linear system <span>$A x = b$</span> in double precision by using a factorization in a lower precision, say single, within a residual correction iteration. This means that one would need to allocate storage for a copy of <span>$A$</span> is the lower precision and factor that copy. </p><p>Then one has to determine what the line <span>$d = (LU)^{-1} r$</span> means. Do you cast <span>$r$</span> into the lower precision before the solve or not? <strong>MultiPrecisionArrays.jl</strong> provides data structures and solvers to manage this. </p><p>Here&#39;s a simple Julia function for IR that</p><pre><code class="nohighlight hljs">&quot;&quot;&quot;
IR(A,b)
Simple minded iterative refinement
Solve Ax=b
&quot;&quot;&quot;
function IR(A, b)
    x = zeros(length(b))
    r = copy(b)
    tol = 100.0 * eps(Float64)
    #
    # Allocate a single precision copy of A and factor in place
    #
    A32 = Float32.(A)
    AF = lu!(A32)
    #
    # Give IR at most ten iterations, which it should not need
    # in this case
    #
    itcount = 0
    # The while loop will get more attention later.
    while (norm(r) &gt; tol * norm(b)) &amp;&amp; (itcount &lt; 10)
        #
        # Store r and d = AF\r in the same place.
        #
        ldiv!(AF, r)
        x .+= r
        r .= b - A * x
        itcount += 1
    end
    return x
end</code></pre><p>As written in the function, the computation of the defect uses <code>ldiv!</code> to compute <code>AF\r</code>. This means that the two triangular factors are stored in single precision and interprecision transfers are done with each step in the factorization. While that <code>on the fly</code> interprecision  transfer is an option, and is needed in many situations, the default is to downcast <span>$r$</span> to low precision, do the solve entirely in low precision, and the upcast the result. The code for that looks like</p><pre><code class="nohighlight hljs">normr=norm(r)
ds=Float32.(r/normr)
ldiv!(AF, ds)
r .= Float64.(ds)*normr</code></pre><p>The scaling by <code>1.0/normr</code> helps to avoid underflow, which is most important when the low precision is <code>Float16</code>. We will discuss interprecision  transfer costs later.</p><h2 id="Integral-Equations-Example"><a class="docs-heading-anchor" href="#Integral-Equations-Example">Integral Equations Example</a><a id="Integral-Equations-Example-1"></a><a class="docs-heading-anchor-permalink" href="#Integral-Equations-Example" title="Permalink"></a></h2><p>The submodule <strong>MultiPrecisionArrays.Examples</strong> has an example which we will  use for most of the documentation. The function <code>Gmat(N)</code> returns the trapezoid rule discretization of the Greens operator  for <span>$-d^2/dx^2$</span> on <span>$[0,1]$</span> with homogeneous Dirichlet boundary conditions.</p><p class="math-container">\[G u(x) = \int_0^1 g(x,y) u(y) \, dy \]</p><p>where</p><p class="math-container">\[g(x,y) = 
    \left\{\begin{array}{c}
        y (1-x) ; \ x &gt; y\\
        x (1-y) ; \ x \le y
    \end{array}\right.\]</p><p>The eigenvalues of <span>$G$</span> are <span>$1/(n^2 \pi^2)$</span> for <span>$n = 1, 2, \dots$</span>.</p><p>The code for this is in the <strong>/src/Examples</strong> directory.  The file is <strong>Gmat.jl</strong>.</p><p>In the example we will build a matrix <span>$A = I - \alpha G$</span>. In the examples we will use <span>$\alpha=1.0$</span>, a very well conditioned case, and <span>$\alpha=800.0$</span> This latter case is very near singularity.</p><p>We will solve a linear system with both double precision <span>$LU$</span> and an MPArray and compare execution time and the quality of the results.</p><p>The example below compares the cost of a double precision factorization to a MPArray factorization. The <code>MPArray</code> structure has a high precision and a low precision matrix. The structure we will start with  is</p><pre><code class="nohighlight hljs">struct MPArray{TW&lt;:AbstractFloat,TF&lt;:AbstractFloat,TR&lt;:AbstractFloat}
    AH::AbstractArray{TW,2}
    AL::AbstractArray{TF,2}
    residual::Vector{TR}
    sol::Vector{TR}
    onthefly::Bool
end</code></pre><p>The structure also stores the residual and a local copy of the solution in precisoin <code>TR</code>. Typically <code>TR = TW</code>. The <code>onthefly</code> Boolean tells the solver how to do the interprecision transfers. The easy way to get started is to use the <code>mplu</code>  command directly on the matrix. That will build the MPArray, follow that with the factorization, and put in all in a structure that you can use with <code>\</code>.</p><p>While we document <code>MPArray</code> and other multiprecision data structures, we do not export the constructors. You should use the multiprecision factorizations instead.</p><p>Now we will see how the results look. In this example we compare the result with iterative refinement with <code>A\b</code>, which is LAPACK&#39;s LU.  As you can see the results are equally good. Note that the factorization object <code>MPF</code> is the output of <code>mplu</code>. This is analogous to <code>AF=lu(A)</code> in LAPACK.</p><p>You may not get exactly the same results for the examples on different hardware, BLAS, versions of Julia or this package.  I am still playing with the termination criteria and the iteration count could grow or shrink as I do that.</p><pre><code class="nohighlight hljs">julia&gt; using MultiPrecisionArrays

julia&gt; using MultiPrecisionArrays.Examples

julia&gt; using BenchmarkTools

julia&gt; N=4096;

julia&gt; G=Gmat(N);

julia&gt; A = I - G;

julia&gt; MPF=mplu(A); AF=lu(A);

julia&gt; z=MPF\b; w=AF\b;

julia&gt; ze=norm(z-x,Inf); zr=norm(b-A*z,Inf)/norm(b,Inf);

julia&gt; we=norm(w-x,Inf); wr=norm(b-A*w,Inf)/norm(b,Inf);

julia&gt; println(&quot;Errors: $ze, $we. Residuals: $zr, $wr&quot;)
Errors: 5.55112e-16, 6.68354e-14. Residuals: 6.66134e-16, 6.68354e-14
</code></pre><p>So the results are equally good.</p><p>The compute time for <code>mplu</code> should be a bit more than half that of <code>lu!</code>. The reason is that <code>mplu</code> factors a low precision array, so the factorization cost is cut in half. Memory is a different story. The reason is that both <code>mplu</code> and <code>lu!</code> do not allocate storage for a new high precision array, but <code>mplu</code> allocates for a low precision copy, so the memory and allocation cost for <code>mplu</code> is 50%  more than <code>lu</code>. </p><pre><code class="nohighlight hljs">julia&gt; @belapsed mplu($A)
8.60945e-02

julia&gt; @belapsed lu!(AC) setup=(AC=copy($A))
1.42840e-01

# And now for the solve times.

julia&gt; @belapsed ldiv!($AF,bb) setup=(bb = copy($b))
4.79117e-03

julia&gt; @belapsed $MPF\$b
2.01195e-02</code></pre><p>So the total solve time is less, but the <span>$O(N^2)$</span> work is not zero.</p><p>It is no surprise that the factorization in single precision took roughly half as long as the one in double. In the double-single precision case, iterative refinement is a great example of a time/storage tradeoff. You have to store a low precision copy of <span>$A$</span>, so the storage burden increases by 50% and the factorization time is cut in half. The advantages of IR increase as the dimension increases. IR is less impressive for smaller problems and can even be slower</p><pre><code class="nohighlight hljs">julia&gt; N=30; A=I + Gmat(N); 

julia&gt; @belapsed mplu($A)
5.22217e-06

julia&gt; @belapsed lu!(AC) setup=(AC=copy($A))
3.70825e-06</code></pre><h2 id="Options-and-data-structures-for-mplu"><a class="docs-heading-anchor" href="#Options-and-data-structures-for-mplu">Options and data structures for mplu</a><a id="Options-and-data-structures-for-mplu-1"></a><a class="docs-heading-anchor-permalink" href="#Options-and-data-structures-for-mplu" title="Permalink"></a></h2><p>Here is the source for <code>mplu</code></p><pre><code class="nohighlight hljs">&quot;&quot;&quot;
mplu(A::AbstractArray{Float64,2}; TF=Float32, onthefly=false)

Combines the constructor of the multiprecision array with the
factorization. 
&quot;&quot;&quot;
function mplu(A::AbstractArray{TW,2}; TF=Float32, onthefly=nothing) where TW &lt;: Real
#
# If the high precision matrix is single, the low precision must be half.
#
(TW == Float32) &amp;&amp; (TF = Float16)
#
# Unless you tell me otherwise, onthefly is true if TF is half
# and false if TF is single.
#
(onthefly == nothing ) &amp;&amp; (onthefly = (TF==Float16))
MPA=MPArray(A; TF=TF, onthefly=onthefly)
MPF=mplu!(MPA)
return MPF
end</code></pre><p>The function <code>mplu</code> has two keyword arguments. The easy one to understand is <code>TF</code> which is the precision of the factorization. Julia has support for single (<code>Float32</code>) and half (<code>Float16</code>) precisions. If you set <code>TF=Float16</code> then low precision will be half. Don&#39;t do that unless you know what you&#39;re doing. Using half precision is a good way to get incorrect results. Look at the section on <a href="#half-Precision">half precision</a> in this Readme for a bit more bad news.</p><p>The other keyword argument is <strong>onthefly</strong>. That keyword controls how the triangular solvers from the factorization work. When you solve</p><p class="math-container">\[LU d = r\]</p><p>The LU factors are in low precision and the residual <span>$r$</span> is in high precision. If you let Julia and LAPACK figure out what to do, then the solves will be done in high precision and the entries in the LU factors will be converted to high precision with each binary operation. The output <span>$d$</span> will be in high precision. This is called interprecision transfer on-the-fly and <code>onthefly = true</code> will tell the solvers to do it that way. You have <span>$N^2$</span> interprecision transfers with each solve and, as we will see, that can have a non-trivial cost.</p><p>When low precision is Float32, then the default is (<code>onthefly = false</code>). This converts <span>$r$</span> to low precision, does the solve entirely in low precision, and then promotes <span>$d$</span> to high precision. You need to be careful to avoid overflow and, more importantly, underflow when you do that and we scale <span>$r$</span> to be a unit vector before conversion to low precision and reverse the scaling when we promote <span>$d$</span>. We take care of this for you.</p><p><code>mplu</code> calls the constructor (<code>MPArray</code>) for the multiprecision array. The constructor allocates storage for a low precision copy of <span>$A$</span> and then factors the low precision matrix. In some cases, such as nonlinear solvers, you will want to separate the constructor and the factorization. When you do that remember that <code>mplu!</code> overwrites the low precision copy of <code>A</code> with the factors. The factorization object is different from the multiprecision array, even though they share storage. This is just like <code>lu!</code>.</p><h2 id="Memory-Allocations-for-mplu"><a class="docs-heading-anchor" href="#Memory-Allocations-for-mplu">Memory Allocations for mplu</a><a id="Memory-Allocations-for-mplu-1"></a><a class="docs-heading-anchor-permalink" href="#Memory-Allocations-for-mplu" title="Permalink"></a></h2><p>The memory footprint of a multiprecision array is dominated by the high precision array and the low precision copy. The allocations of</p><pre><code class="nohighlight hljs">AF1=lu(A)</code></pre><p>and</p><pre><code class="nohighlight hljs">AF2=mplu(A)</code></pre><p>are very different. Typically <code>lu</code> makes a high precision copy of <code>A</code> and factors that with <code>lu!</code>. <code>mplu</code> on the other hand, uses <code>A</code> as the high precision matrix in the multiprecision array structure and the makes a low precision copy to send to <code>lu!</code>. Hence <code>mplu</code> has half the allocation burden of <code>lu</code>.</p><p>That is, of course misleading. The memory-efficient  way to apply <code>lu</code> is to overwrite <code>A</code> with the factorization using</p><pre><code class="nohighlight hljs">AF1=lu!(A).</code></pre><p><code>mplu</code> uses an analog of this approach internally. <code>mplu</code> first builds an <code>MPArray</code> structure with</p><pre><code class="nohighlight hljs">MPA = MPArray(A)</code></pre><p>which makes <code>A</code> the high precision matrix and also makes a low precision copy. This is the stage where the extra memory is allocated for the the low precision copy. Then <code>mplu</code> computes the factorization of the low precision matrix with <code>mplu!</code> to construct the factorization object.</p><pre><code class="nohighlight hljs">MPF = mplu!(MPA).</code></pre><p>So, the function <code>mplu</code> simply applies <code>MPArray</code> and follows that  with <code>mplu!</code>. </p><h2 id="Other-IR-software-in-Julia"><a class="docs-heading-anchor" href="#Other-IR-software-in-Julia">Other IR software in Julia</a><a id="Other-IR-software-in-Julia-1"></a><a class="docs-heading-anchor-permalink" href="#Other-IR-software-in-Julia" title="Permalink"></a></h2><p>The package <a href="https://github.com/RalphAS/IterativeRefinement.jl">IterativeRefinement.jl</a> is an implementation of the IR method from (<a href="References/#dongarra_1983">J.J.Dongarra <em>et al.</em>, 1983</a>).</p><p>The unregistered package <a href="https://github.com/bvieuble/Itref.jl">Itref.jl</a> implements IR and the GMRES-IR method from  (<a href="References/#amestoy_2024">Amestoy <em>et al.</em>, 2024</a>) and was used to obtain the numerical results in that paper. It does not provide the data structures for preallocation that we do and does not seem to have been updated lately.</p></article><nav class="docs-footer"><a class="docs-footer-nextpage" href="Half_1/">Half Precision and Krylov-IR »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.13.0 on <span class="colophon-date" title="Tuesday 24 June 2025 17:39">Tuesday 24 June 2025</span>. Using Julia version 1.10.9.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
